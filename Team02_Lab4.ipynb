{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "301d67f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Teja\n",
      "[nltk_data]     Nikhil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4757d763",
   "metadata": {},
   "source": [
    "### Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "a30fe4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Directory = r\"D:\\Sem-6\\21AIE312 - DL\\Project Data\\DL-Dataset\\CSV files\"\n",
    "data_dir = os.listdir(Directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "771745ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19CSE305_2022_Quiz-2 (2000.19CSE305.B)(1-57).csv',\n",
       " '19CSE305_2022_Quiz-2 (2000.19CSE305.C)(1-59).csv',\n",
       " '21DS602_2022-23_Quiz-2 (21DS602-Machine Learning)(1-30).csv',\n",
       " '21DS602_2022-23_Quiz1 (21DS602-Machine Learning)(1-33).csv',\n",
       " '21DS602_2022-23_Quiz3(1-34).csv',\n",
       " 'BA Quiz 1 (19CSE352-Business Analytics)(1-73).csv',\n",
       " 'Business Analytics Quiz-2 (19CSE352-Business Analytics)(1-72).csv',\n",
       " 'Class Quiz 1 (July 29, 2021) (19CSE305-Machine Learning)(1-127).csv',\n",
       " 'ML (21DS602_21DS644) Quiz 1 (Machine Learning - 21CS644_21DS602)(1-56).csv',\n",
       " 'ML Quiz (Machine Learning - 21CS644_21DS602)(1-56).csv',\n",
       " 'Quiz 1 (2000.19CSE305.A)(1-66) (1).csv',\n",
       " 'Quiz 1 (2000.19CSE305.C)(1-68) (1).csv',\n",
       " 'Quiz 1(1-64) (1).csv',\n",
       " 'Quiz 3 - 20211012 (19CSE305-Machine Learning)(1-139).csv']"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "46f1be29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.ceil(1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "25ee6cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_processed_csv(file_directory):\n",
    "    df = pd.read_csv(file_directory)\n",
    "    cols = df.columns\n",
    "    for i in range(0,len(cols),2):\n",
    "#         print(cols[i+1])\n",
    "        df[cols[i+1]] = df[cols[i+1]].astype(\"float\")\n",
    "        df[cols[i+1]]=[int(np.ceil(j)) for j in list(df[cols[i+1]])]\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "6db9d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Count(points_list):\n",
    "    points = set(points_list)\n",
    "    counts = [points_list.count(i) for i in points]\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "98b6c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data_Count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "464c7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions = []\n",
    "QA = dict()\n",
    "data_count = []\n",
    "for i in data_dir:\n",
    "#   print(i)\n",
    "    data_path = os.path.join(Directory, i)\n",
    "    csv_read = read_processed_csv(data_path)\n",
    "    cols = csv_read.columns\n",
    "#   print(cols)\n",
    "    for j in range(0,len(cols),2):\n",
    "        Questions.append(cols[j])\n",
    "        Answer_grades = dict()\n",
    "        answers = list(csv_read[cols[j]])\n",
    "        points = list(csv_read[cols[j+1]])\n",
    "        data_count.append(Data_Count(points))\n",
    "        for (k,l) in zip(answers, points):\n",
    "            Answer_grades[k] = l\n",
    "        QA[cols[j]] = Answer_grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "c0439a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_count = 0\n",
    "ones_count = 0\n",
    "twos_count = 0\n",
    "for i in data_count:\n",
    "    zero_count = zero_count + i[0]\n",
    "    ones_count = ones_count + i[1]\n",
    "    if len(i)==3:\n",
    "        twos_count = twos_count + i[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "0e79242b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "708"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "43592a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1086"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "a6f6959d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "4af5477d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_questions = len(Questions)\n",
    "num_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "99547817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data count is a list of count of different grades with their indices indicating the grade\n",
    "len(data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "efaa8733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name 2 approaches used for solving multi-class classification problem with SVM.\\xa0': {'Stochastic Gradient Descent, gradient': 0,\n",
       "  'linear approach, sqwigy approach,polynomial function, kernel function, sigmoid function': 0,\n",
       "  '1.  Gradient Descent 2. Tabu method': 0,\n",
       "  'the two approaches are 1. One versus One approach 2. One versus remaining approach': 1,\n",
       "  'One to One approach(n(n-1)/2), one to all approach(n)': 1,\n",
       "  'one against another, one against all': 1,\n",
       "  'One class with another class (no. of models=n(n-1)/2), one against all classes (no. of models=n) [n=no.of classes]': 1,\n",
       "  '1 vs 1 and 1 vs 2': 1,\n",
       "  'One vs One and one vs all': 1,\n",
       "  'gradient decent is one of the approach , back propogation': 0,\n",
       "  'one vs one, one vs many': 1,\n",
       "  'one vs one approach and one vs all approach.': 1,\n",
       "  'one to many comparison where one class is taken and it is compared with the other classes in the dataset. The other way of solving the multi classification is by one to one where we compare each class with the other class.': 1,\n",
       "  'One to Many (in this one class is taken as +ve class and all others are taken as -ve) and One to One (in this one class is +ve and anther class is -ve and this is done for all classes)': 1,\n",
       "  'gradient descent approach, back propagation, decision tree': 0,\n",
       "  'One against other, One vs all': 1,\n",
       "  'multiple class against one class, each class comparison with another class': 1,\n",
       "  'gradient descent and back propogation': 0,\n",
       "  'one vs one approach, one vs rest approach': 1,\n",
       "  'one vs one ; one vs rest of them': 1,\n",
       "  ' Stochastic Gradient Descent algorithm': 0,\n",
       "  'gradient descent method , decision tree method , kernel method': 0,\n",
       "  'decision tree method, gradient decendent method': 0,\n",
       "  'one vs rest approach, one vs one approach': 1,\n",
       "  'one vs one , one vs many': 1,\n",
       "  'classification and regression': 0,\n",
       "  'gradient descent and FFNN': 0,\n",
       "  'gradient descent , decision tree ': 0,\n",
       "  'Stochastic Gradient Descent,linear approach': 0,\n",
       "  'stochastic gradient Descent,linear aproach': 1,\n",
       "  'one vs one , and one vs rest': 1,\n",
       "  'Regression': 0,\n",
       "  'One vs All , One to One': 1,\n",
       "  'one vs rest, one vs one': 1,\n",
       "  'Classification and Regression.': 0,\n",
       "  'one against other,one vs one': 1,\n",
       "  '1)one vs one 2) one vs rest are the two approaches.': 1,\n",
       "  'SVM(support vector machines),linear approach,squeezy approach': 0,\n",
       "  '1) One vs One,  2) One vs rest of them': 1,\n",
       "  'one vs one and one vs rest': 1,\n",
       "  'One against another and One vs all are the 2 approaches for solving multi-class classification problem with SVM. In one against another approach, n*(n-1)/2 svm models will be constructed. In one vs all approach, n models will be constructed where n is the number of classes': 1,\n",
       "  'classifcation and regresstion': 0,\n",
       "  'one vs one and one vs rest these are the two approaches used for solving multi class classification problem with svm.': 1,\n",
       "  'construct svm model for every pair if there are n classes then n*(n-1)/2 svm models has to be constructed , decision tree method': 0,\n",
       "  'One to one, one to every other': 1,\n",
       "  'one vs one, one vs rest and regression': 1,\n",
       "  'one to many approach, one against one approach. Using different kernel functions like sigmoidal. Using SVM poly approach.': 1,\n",
       "  '1. One graph can have only 2 classes, so pair one class with another and make a graph for each pair. Then find the linear separator for each, and find the best one. ': 0,\n",
       "  'one vs one , one vs rest': 1,\n",
       "  '1The multiclass problem is broken down to multiple binary classification cases, which is also called oneto one. and teh other method is using kernal functions.': 1,\n",
       "  ' , 1vs1': 1,\n",
       "  'one-vs-one, one vs all': 1,\n",
       "  'linear approach, Squisy approach ': 0,\n",
       "  'one versus one': 1,\n",
       "  'Classification and Regression': 0},\n",
       " 'Explain the role played by Kernel function in SVM. How does the Kernel function achieve this?\\xa0': {'in svm kernal plays as flag': 0,\n",
       "  'kernel function have the subsets of the data in its own dataset. kernel function achieve this by using e^-gamma(a-b)^2': 0,\n",
       "  'Kernel function is used to transform the attributes of a given dataset into a different form so that SVM prediction can be applied.': 0,\n",
       "  'provides window to manipulate data': 0,\n",
       "  'Kernel function helps us to classify non-linearly separable classes. The kernel function achieves this by making the classes fit in one suitable dimension where they are seperable and can be classified using a hyperplane. The kernel function is obtained after optimising the function phi(x) = 1/2(W(transpose).W))': 1,\n",
       "  'it transforms the training set of data': 0,\n",
       "  'kernel function provides the window to manipulate the data. kernel func = (xbar) ^T.(xbar)': 0,\n",
       "  'role': 0,\n",
       "  'The training set of data is typically transformed by a kernel function to enable a non-linear decision surface to transform to a linear equation in a larger number of dimension spaces.': 1,\n",
       "  'kernal function is basically a collection of mathematical equation like polynomial radial etc,kernal function uses linear class separation': 0,\n",
       "  'kernel is a set of mathematical functions to manipulate the data': 0,\n",
       "  'kernel function accepts the input from the user and transforms into desired output': 0,\n",
       "  \"The kernel function is the set of mathematical equations to manipulate the plane so that the data can be classified. Here the kernel is used to separate different functions like radial, polynomial etc. which don't lie in the linearly separable category.\": 0,\n",
       "  'The kernel is a mathematical function that helps us to preserve the dot product. The kernel function helps to convert from one dimension to the other and achieves this using mathematical formulas like sigmoid, polynomial etc': 1,\n",
       "  'the kernel function changes the lower dimensional to higher dimensional space and it is done by using different types of functions': 1,\n",
       "  'kernel - set of mathematical function to manipulate the data.Role - It transforms the training set of data to non-linear': 0,\n",
       "  'used to choose between different activation functions': 0,\n",
       "  'Kernel function is a set of mathematical equations like polynomial, radial etc. It is used for linear class separation ': 1,\n",
       "  'kernel function used for optimization in SVM. It is used for maximize the separation between classes.  k(xi,xj)=(xj^T).xi': 1,\n",
       "  'kernel is used to change dimensions from low to high in svm.kernel achieve using hyperplane': 1,\n",
       "  'helps in transforming the training set so that a non-linear decision surface is able to transform to a linear equation by converting a lower dimension space to higher dimension space': 1,\n",
       "  'it helps in transforming training set , it helps in transforming lower dimension space to higher dimension space, so that non linear decision surface is able to transform a linear equation.': 1,\n",
       "  'it is used for optimization in svm ': 0,\n",
       "  'To transform non-linear data into linear form': 1,\n",
       "  'Kernel maximizes the separation between two classes': 1,\n",
       "  'it transforms the data set to non-linear descion tree': 0,\n",
       "  'the kernel is a mathematical function used to train SVM model for linear class separation': 1,\n",
       "  'kernel function changes the function from lower dimension to higher dimension ': 1,\n",
       "  'In SVM the kernel function helps in decision and creating boundary.': 0,\n",
       "  'In kernal transforms the training set of data ,so that a non-linear decision surface is able to transform to a linear equation in a higher number of dimension spaces,and group of linear functions is called kernal ': 1,\n",
       "  'in kernal transfors the training set of data s thaht a non linear decision is able to transform to a linear equation in a higher number of dimension space f ': 1,\n",
       "  'transforming the training set of data so that non-linear decision is able to transform to a linear equation': 1,\n",
       "  'Kernel Function is function used in SVM to transform the training dataset into non linear dataset and transforms into linear spaces': 1,\n",
       "  'It helps in converting non-separable data to separable data using mathematical equations and models': 1,\n",
       "  'transforms the training set of data so that a non-linear decision surface is able to transform to a linear equation': 1,\n",
       "  'it helps to determine the shape of hyperplane and decision tree, Kernels are form in non- linear seperable example.': 1,\n",
       "  'kernel function is most suitable': 0,\n",
       "  'kernel function in svm is used for optimizing the data.this function helps the svm classifier to get efficient,accurate results.': 0,\n",
       "  'Kernel methods work on mapping data into a high-dimensional space. to get accurate data In this space, the data can be more easily separated into classes or clusters. ': 1,\n",
       "  'it is used in tranforming the training set': 0,\n",
       "  'It is used to transfer the traning set of data. kernal is used to manupilate data': 0,\n",
       "  'Kernel function is a set of mathematical functions to manipulate the data. It transforms the lower dimensional data into a higher dimensional data to find the hyperplane that can successfully classify the data. Examples of kernel functions include polynomial function, radial basis function, sigmoid function, etc.': 1,\n",
       "  'role of kernal function in the svm is it changes the set of data into non-linear form to linear data form ': 1,\n",
       "  'the role of the kernel is used for optimization of data set in svm. it helps to get accrate solutions.': 0,\n",
       "  'If the classes are not linearly seperable in the given space by using kernel function we can increase the space which may leads to separate the classes linearly.rly': 1,\n",
       "  'using kernel function we can transform non linear data to linear form': 1,\n",
       "  'It translates non seperable data to seperable data. example in polynomial kernel functions it uses that expression to achieve this task': 1,\n",
       "  'kernel function divide the groups then we can form margins and between margins hyperplan': 0,\n",
       "  'Kernel function gives the optimized function for weight updating. w transpose dot w is an example. Kernel achieves this function by replacing the regular weight updation..': 0,\n",
       "  'If the dataset is linearly inseparable, it is better to use kernel in SVM. It makes the non-linear data to a linear data by using a non-linear function. It fits a non-linear function on the data which can separate the data. ': 1,\n",
       "  'kernel is used to set functions in svm and  to manipulate data': 0,\n",
       "  'Kernel is a  mathematical function used in Support Vector Machine  that gives us the window to manipulate the data.they perfome mathematical calculations and come to  a conclusion accordingly.': 0,\n",
       "  'In Kernel function it converts low dimension to high dimension ': 0,\n",
       "  'kernel helps making linearly inseparable data into linearly separable.': 1,\n",
       "  'they help us in determining  the shape of the hyperplane and also helps in making  decision boundry.': 0,\n",
       "  'noo': 0,\n",
       "  'Kernel is used because a set of mathematical functions used in support vector machine which provides window to manipulate the data. Kernel function achieves this by transforming the training set of data where a non linear decision surface is able to transform to a linear equation in a higher number of dimension spaces.': 1,\n",
       "  'Kernel in SVM used to manipulate and it transforms the train data from a non-linear decision data is able to transform to a linear seperable in many dimension spaces.': 1},\n",
       " 'Explain a scenario (in not more than 2 sentences) when information gain is zero.': {\"before split and after split same entropy . H(s)=H(S')  .H(S) is the entropy of before split.H(S') is the entropy after split.\": 1,\n",
       "  'Information gain is zero when example: Sun rises in the east. This statement is very obvious. Therefore no information gain.': 0,\n",
       "  'When the information gain is 0 it means that the probabilities belongs to same class. Higher the information gain lower is the uncertainty of decision.': 0,\n",
       "  ' There is no change in the entropy after split. If the data consists of only one class.': 0,\n",
       "  'there is no change in entropy after split. entropy before split and after split will be same': 1,\n",
       "  'Information gain is the measure of impurity. If its a zero then there is only one class for which there is no need to buid a mo fr wich the results are obvious': 0,\n",
       "  'When the entropy of parent and child node is same we get 0 information gain. If the features cannot be divided further(leaf node) then information gain is 0.': 1,\n",
       "  'the information gain is zero when the entropy is low i.e the low entropy indicates data purity i.e the class is well defined.when entropy reduces the information gain is high.': 0,\n",
       "  'We know that Information gain is the difference between the Entropy(Parent) i.e impurity degree before splitting and Entropy(child) i.e impurity degree after splitting. Keep in mind the N . The information gain will be zero when the entropy value is zero ; stating all the values belongs to same class. If the entropy value is 1 , there there is a posibility of all the values are equally divided. But when the entropy value is not uniform , there is no variance than the information gain will have no role. ': 2,\n",
       "  'when there is no impurity present and information gain value in not zero': 0,\n",
       "  \"Happens when the feature entropy is same as the total entropy. This happens when a particular attribute values have equal number of possible outcome values ie. eg. for a bi,class classification, a particular feature has 50% 'yes', and 50% 'no' responses.\": 0,\n",
       "  'Consider a case of two class classification. If the ratio of number of positive and negative class members in all (segregated) child nodes is same as in parent node, then information gain is zero.': 2,\n",
       "  '1. The values of the features are completely independent of each other and of the values of the target variable 2.Where features provide accurate prediction of the target variable': 0,\n",
       "  'if your data contains only one class, you already know what the class is without having seen any attribute values and the information gain will always be 0.': 0,\n",
       "  'When we have only one class, the info gain will be zero': 0,\n",
       "  'if suppose the data  contains only one class, without any attribute values then information gain will be zero.': 0,\n",
       "  'the target set will be having only one class in the dataset, lower the information gain higher is the  is the entropy': 0,\n",
       "  'If  data contains only one class, we already know what the class is without having to see any attribute values and the information gain will always be 0': 0,\n",
       "  '1) A classification algorithm may not be able to leverage all the information that the attributes can provide. 2) A classification algorithm may implement its own attribute selection internally that considers a smaller subset than attribute selection will yield.': 0,\n",
       "  'entropy of a group when all examples belong to the same class information gain will be zero.  or if data has only one class, and the class is seen without having any attribute values and the information gain will always be zero': 0,\n",
       "  'if one class is present then information gain is zero because we already know the class.': 0,\n",
       "  'Information gain is zero if our dataset contains single class,. Only one attribute it is difficult to compare and calculate accuracy . hence the information gain is 0': 0,\n",
       "  'information gain is 0 means H(s) or entropy will be 0. when we already know the probability that all the observations belong to same class then the information gain can become 0': 0,\n",
       "  'Information gain will be zero when there will be no gain of post information on data i.e when dataset contains only one class.': 0,\n",
       "  'when the data set is of only 1 class  so we already know the output without seeing the attributes value , also when features are considired independent of each other the information gain is zero': 0,\n",
       "  'As the node is not good to select for splitting information gain will be too low. Because there the uncertainty of decision is high.': 0,\n",
       "  'When the data set has only 1 class, we already know the output without seeing the values of the attributes.': 0,\n",
       "  '1. When the dataset contains only one class, we already have an idea what the class(output) is without having seen any attribute values. 2. Features are considered independent (isolated) from one another. ': 0,\n",
       "  'if all data instances belong to same class then information gain will be zero, as the entropy is zero.': 0},\n",
       " 'Differentiate between simple and complete linkage proximity measures. Use one or 2 sentences only.': {'Simple linkage is minimum distance of datapoints from two clusters.Complete linkage is the maximum distance of datapoints from two clusters.': 2,\n",
       "  'Simple linkage is the minimum distance between element in one cluster to another cluster. Complete linkage is the maximum distance between element in one cluster to another cluster.': 2,\n",
       "  'In simple the distance between 2 sets is calculated and the minimum distance is used. In complete linkage, after calculating distance the maximum distance is selected.': 2,\n",
       "  'In simple linkage the minimum distance is considered for distance matrix and for Complete linkage maximum distance is considered. ': 2,\n",
       "  'in complete linkage the maximum distance between the two set swill be taken. in simple  minimum distance will be taken': 2,\n",
       "  'Dont know': 0,\n",
       "  'In simple linkage we take min distance between points from 2 clusters. In complete linkage we take max distance between points from two clusters.': 2,\n",
       "  'no': 0,\n",
       "  'Simple Linkage Proximity measure use minimum distance between the points where as complete linkage proximity measures deals with maximum distance between the attributes of values. Based upon minimum distance, groups are formed under Single linkage. Based upon maximum distance of points, groups are linked.': 2,\n",
       "  'In simple proximity measures': 0,\n",
       "  'Single Linkage considers minimum of the values when the cluster pairs are grouped.Complete Linkage uses maximum of the distance between the cluster pairs while grouping.': 2,\n",
       "  'In simple linkage, the proximity between 2 sets in based on the minimum of the pairwise distance measure between the members of the 2 sets. In complete linkage, it is based on the maximum of pairwise distance measure.': 2,\n",
       "  'Simple linkage - find the minimum distance that becomes the distance between set A and set B, Complete linkage - find the maximum distance that becomes the distance between set A and set B': 2,\n",
       "  'Single Linkage is a method that focused on minimum distances or nearest neighbor between clusters meanwhile Complete Linkage concentrates on maximum distance or furthest neighbor between clusters.': 2,\n",
       "  'Single Linkage proximity - measures similarity in most similar pair, wherein complete linkage measures between distant elements': 1,\n",
       "  'complete linkage  is dist. between farthest pair of observations,   Simple linkage is focussed on minimum.': 2,\n",
       "  'simple linkage search for nearest neighbour and complete search for maximum distance': 2,\n",
       "  'Complete linkage distance tends to create compact clusters of clusters, while single linkage tends to add one point at a time to the cluster, creating long stringy clusters. For the Single linkage, two clusters with the closest minimum distance are merged and for the Complete linkage, two clusters with the closest maximum distance are merged.': 2,\n",
       "  '1)Hierarchical clustering, we merge in each step the two clusters whose merger has the smallest diameter, 2) Hierarchical clustering, we merge in each step the two clusters whose two closest members have the smallest distance. ': 1,\n",
       "  'Complete linkage distance is measured between the farthest pair of observations in two clusters, and Simple is the distance between two clusters that is the minimum distance between members of the two clusters.': 2,\n",
       "  'Complete linkage distance is measured between farthest pair of observations in two clusters. Simple linkage is the distance between two clusters having minimum distance between them.': 2,\n",
       "  'Simple linkage calculates minimum distance between two points and Complete linkage calculates maximum distance between two given points .': 2,\n",
       "  'complete linkage is measured as the maximum distance or fathest points of 2 sets become distance of clusters.  whereas single is minimum distance between sets of clusters become distance of clusters': 2,\n",
       "  'Simple linkage proximity measure has minimum distance in their between two members while complete linkage has maximum distance.': 2,\n",
       "  'Simple linkage is the smallest distance between an element in one cluster and an element in the other whereas COMPLETE LINKAGE is the largest distance between an element in one cluster and an element in the other,': 2,\n",
       "  'Single linkage is the  smallest distance between an element in one cluster and an  element in the other. that is :dist(Ki , Kj ) = min(tip, tjq).    omplete linkage: largest distance between an element in one cluster and an  element in the other, that is distance(Ki , Kj ) = max(tip, tjq)': 2,\n",
       "  'complete linkage distance tends to create compact clusters of clusters, while single linkage tends to add one point at a time to the cluster, creating long stringy clusters.': 2,\n",
       "  'simple linkage is a proximity measure that calculates the distance between two clusters by finding the minimum distance between any two clusters. Complete linkage is a proximity measure that calculates the distance between two clusters by finding the maximum distance between any two points in the two clusters.': 2,\n",
       "  'Single Linkage: Proximaty measure that calculates the minimum distance between an element from one cluster point to the element in the the other cluster.  Complete Linkage: Maximum Distance between an element from one cluster point to the element in the the other cluster.': 2,\n",
       "  'simple linkage:similarity of most similar pair in a cluster, complete linkage:considers distance between the most distant elements of each cluster.': 2},\n",
       " 'Explain the scenario of overfitting in k-NN classifier.': {'overfitting': 0,\n",
       "  'Performs well on training data but poor performance for training data': 0,\n",
       "  'When the value of k is small its overfitting, because its  too much specific of the neighbor, that it wont misinterpret unseen data': 1,\n",
       "  'if k=1,test pattern consider only immediate neighbor and based on that class is decided.': 1,\n",
       "  'Overfitting will happen in KNN classifier if the K value taken is small. Since it gets confined to a small space of specific points.': 1,\n",
       "  'if  k that we have chosen is very small in knn classification then it will be over fitted': 1,\n",
       "  'KNN classifier has overfitting when the k value given is smaller and the system would have poor performance': 1,\n",
       "  'When k is low (eg,k=1), it leads to overfitting in kNN.': 1,\n",
       "  'when k value is 1 if all data can be classified': 1,\n",
       "  'Overfitting in kNN occurs when the value of hyper-parameter (k) is small. For example, if k is 1, each training sample defines its own class boundary/sphere of influence.': 1,\n",
       "  'If k value is less then the training accuracy is more and testing accuracy will be less which means overfitting has occurred.': 1,\n",
       "  'If the number of K values are small typically , it causes overfitting . ': 1,\n",
       "  'Supervised Learning technique.': 0,\n",
       "  'When k is small it is a overfitting classifier . The training value is greater than test value .': 1,\n",
       "  'If the number of k will be large near to 10 then it will give overfitting.': 0,\n",
       "  'overfitting means the scenario where the model works well during training but does not perform well during test data comes in.': 0,\n",
       "  'overfitting means that the training data is well fitted but prediction has poor performance': 0,\n",
       "  'in overfitting scenario value of k is small and accuracy fluctuates sharply with change of k value in KNN classifier. ': 1,\n",
       "  'Overfitting in KNN says that the model has a good accuracy on their training model while has a very low performance error in the testing model.': 0,\n",
       "  'when the K value is too low': 1,\n",
       "  'Overfitting imply that the model is well on the training data but has poor performance when new data is coming. the k value will increase that is greater k value ': 0,\n",
       "  'First the datasheet is separated as train and test set . Then we get errors for both train and test data when it is high training error  in comparision the test errors is quite low. this is overfitting.  training data accuracy increaese and test error decreases. ': 0,\n",
       "  'There is high training error and less test error, so the accuracy of training dataset increases.': 0,\n",
       "  'unseen cannot be supported, not generalized': 0,\n",
       "  'A small value of k can lead to overfitting': 1,\n",
       "  'Overlifting in knn means, it gives good performance or accuracy for the training data. But when new data is coming it will not work efficiently as it worked in training data.': 0,\n",
       "  'When the k value is less, there is overfitting of the data': 1,\n",
       "  'When the model is going good on the training data but for bad performance when using new data can be refers as overfitting. When maximum classification branches also results overfitting in decision tree. ': 0,\n",
       "  'when k is more then it will be overfitting. ie when the model is trained with so much data, it learns from the noise. ': 0,\n",
       "  'if k value is small it is overfitting, if distance is too small.': 1,\n",
       "  'Do not know': 0,\n",
       "  'If k is small it leads to overfitting, for new data the model will not provide good result.': 1,\n",
       "  'It is a simple algorithm and k nearest data points and use their labels to predict the labels of new data points': 0},\n",
       " 'State one difference between Gradient Descent and Stochastic Gradient Descent optimizers.': {'In gradient descent': 0,\n",
       "  'Stochastic Gradient descent is faster than Gradient Descent. ': 0,\n",
       "  'For Gradient descent learning rate is less than one, For Stochastic its greater than 1': 0,\n",
       "  'in gradient descent, sum of squared error is calculated for all input and weight is updated. In Stochastic Gradient descent sum of squared error of each input is considered and based on that weight is updated': 1,\n",
       "  'Stochastic gradient descent finds the loss for each and every data point and is very slow. Whereas gradient descent takes batch of data and finds loss for for the batch every iteration. It is therefore fast.': 1,\n",
       "  'stochastic descent is faster convergence compared to gradient discent ': 0,\n",
       "  'Stochastic GD is suitable for large datasets compared to GD , SGD increases model accuracy better than GD': 0,\n",
       "  'In Gradient Descent, in each step ,calculations are performed on all the training patterns. In stochastic Gradient Descent, a training pattern is selected random, and calculations are performed on that training pattern, instead of all the patterns.': 1,\n",
       "  'in stochastic error is calculated for one by one weight, but in gradient sum of weight error calculated': 1,\n",
       "  'For gradient descent, weight updation happens after the forward pass of all training samples is over. In stochastic gradient descent, weight updation occurs after forward pass of each training sample.': 1,\n",
       "  'na': 0,\n",
       "  'In Gradient Descent we use Sum Square Error and in Stochastic Gradient Descent we dont use Sum Square error.': 1,\n",
       "  'Stochastic Gradient Descent is faster and better for big datasets than Gradient Descent. Since the gradient is only computed for one random point per iteration, the updates have more volatility.': 1,\n",
       "  'number of iterations differ ': 0,\n",
       "  'In comparision to simple gradient stochastic gradient updates the parameter after every step.': 1,\n",
       "  'stochastic gradient is a probabilistic approximation of gradient descent.': 0,\n",
       "  'Stochastic Gradient Descent is faster compared to Gradient Descent optimizers': 0,\n",
       "  'gradient descent is done on each point but stochastic gradient descent is done for one random point on each iteration': 1,\n",
       "  'Stochastic Gradient Descent is much faster and more suitable to large data set and has more iterations.': 0,\n",
       "  'SGD is faster and useful in big datasets': 0,\n",
       "  'Stochastic Gradient Descent is much faster, and more suitable to large-scale datasets.': 0,\n",
       "  'in no. of iterations.': 0,\n",
       "  'The difference is observed in iterations': 0,\n",
       "  'Stochastic Gradient Descent is much faster, and more suitable to large-scale datasets': 0,\n",
       "  'Stochastic Gradient Descent is much faster, and more suitable to large-scale datasets compared to gradient descent': 0,\n",
       "  'Stochastic gradient descent works faster than gradient descent. Also it works more efficiently in big datasets.': 0,\n",
       "  'whole training sample is used in gradient descent and single training sample is used in stochastic Gradient Descent': 1,\n",
       "  'Batch Gradient Descent involves calculations over the full training set at each step as a result of which it is very slow on very large training data.SGD tries to solve the main problem in Batch Gradient descent which is the usage of whole training data to calculate gradients at each step.': 0,\n",
       "  'in  gradient descent use for finding local minima of a function in sum square error and stochastic gradient descent takes only a small part for calculating the gradiant': 1,\n",
       "  'GD used whole training data,so gets stuck. SGD uses batch of the training sample, so it is fast.': 1,\n",
       "  'Gradient Descent optimizers is deterministic in nature whereas stochastic gradient descent is Stochastic in nature.': 0,\n",
       "  'Gradient Descent - It uses entire data set to find optimal solution but stochastic uses one best example at a time to find optimal solution': 1,\n",
       "  ' gradient descent -Gives optimal solution given sufficient time to converge. stochaastic descent optimizers-Gives good solution but not optimal.': 0},\n",
       " 'Explain a scenario where F1 is used for better result analysis in place of accuracy measure.': {'it can be used in credit card fraud detection or in supply chain fraud detection where F1 is used for better result analysis in place of accuracy measure': 1,\n",
       "  'we use f1 score  when the cost of false negatives and false positives are different.': 1,\n",
       "  \" In scenarios where classes are imbalanced, accuracy may not be the best metric to evaluate the performance of a classifier. In this case, F1-score is often used as it gives equal weight to both precision and recall, which are measures of the classifier's ability to correctly identify positive examples and avoid false positives, respectively.In this case, a classifier that has high recall but low precision would be preferred over one that has high precision but low recall. In this scenario, F1-score can be used for better result analysis as it takes both precision and recall into account.\": 1,\n",
       "  'The case where the accuracy gives are good values but the balance of precision and recall might vary. So F1 Score gives us the balancing precision and recall for positive class, in binary classification we can use this , for multiclass problem we talk about only cost function. ': 1,\n",
       "  'For example for testing whether a person is covid positive or not. If the false negative is high, then this could impact the huge population. Therefore here F1 is better than accuracy measure.': 1,\n",
       "  'F1 score does a better job when there is class imbalance and the cost of mis-classification is symmetric. This is because the denominator is partitioned for positive and negative classes and then combined later into a single score, which works better than accuracy when there is class imbalance.': 1,\n",
       "  'In medical diagnosis a high F1 score would indicate that the classifier is able to correctly identify a high proportion of positive cases while minimizing the number of false negatives': 0,\n",
       "  ' when we have an uneven class distribution f1 is better': 1,\n",
       "  'Accuracy does not consider the mis-classification in terms of  FP and FN. F1-score  considers both Precision and Recall.  For testing a patient for Covid, cost of FP and FN are also important and should be considered.In this scenario, F1 score will be a better measure, since it considers the misclassifications also.': 1,\n",
       "  'balancing precision  and recall on a positive class, it sums up predictive performance of a model.': 0,\n",
       "  'In case of email spam whether the mail is a spam or not,  a high F1 score would indicate that the classifier is able to correctly identify a high proportion of positive cases while minimizing the number of false negatives cases.': 0,\n",
       "  'A scenario where a model is trained to detect fraudulent transactions in a bank, a high F1 score would indicate that the model has a good balance of precision and recall.': 1,\n",
       "  'In the case where a balance between precision and recall on the positive class is required while accuracy looks at correctly classified observations both positive and negative.': 0,\n",
       "  'F1 considers both precision and recall. F1 score is used when the classes are imbalanced. ': 1,\n",
       "  ' F1 score is balancing precision and recall on the positive class  ': 0,\n",
       "  'Binary classification': 0,\n",
       "  'to give equal imporance to recall and precision': 0,\n",
       "  'foe F1 ,Beta  value is one . precision and recall gets equal importance': 0,\n",
       "  'precision and recall having equal importance': 0,\n",
       "  'Dont know sir': 0,\n",
       "  'It is a measure of accuracy on a dataset. It is used to find binary classification systems, Positive or negative. rains or not, pregnant or not, cancer or not.': 1,\n",
       "  'f1 is the harmonic mean of precision and recall so it can be used when these two values are good enough': 0,\n",
       "  'class imbalanced cases we use f1 score': 1,\n",
       "  'F1 score considers precision and accuracy it is better to use that in place of accuracy measure': 0,\n",
       "  'F1 is used when the classes are imbalanced.': 1,\n",
       "  'Where we need both Accuracy and Recall - both are important ': 0,\n",
       "  'F1-score is one of the most important evaluation metrics in machine learning.': 0,\n",
       "  'when we have uneven class distribution it is better ': 1,\n",
       "  'During class imbalance F1 score can be used insread of other measures.  compared to accuracy . ': 1,\n",
       "  'F1 score is 1 only when precision and recall are both 1. it is also known as balanced fscore. whenever there is class imbalance is found': 1,\n",
       "  'when we need more false positives, we will use f1 score, when there is class imbalance.': 1,\n",
       "  'F1 score is balancing precision and recall on the positive class while accuracy looks at correctly classified observations both positive and negative.': 0,\n",
       "  'in case of uneven class distribution': 1,\n",
       "  'F1-score is harmonic mean of precision(gives importance for FP) and recall(gives importance for FN) score which can be used for classifying imbalanced dataset. But accuracy measure can be used for classifying balanced dataset.': 1},\n",
       " 'How may AUROC plot be used to identify the best classifier from a multitude of models?\\xa0': {'2 AUROC plot can be used to identify the best classifier from multitude of models': 0,\n",
       "  '2': 0,\n",
       "  'A perfect classifier would have an AUROC of 1, while a random classifier would have an AUROC of 0.5.': 1,\n",
       "  'One class versus the rest can be used. 1 would be sufficient.': 0,\n",
       "  'AUROC plot helps in finding the best classifier since it gives the plot between False positive and true positive. If the value is 1 it is perfect classification, if 0.5 it is random classification, if less than that it is poor. Using this we can compare the best model.': 1,\n",
       "  'One plot per model.': 0,\n",
       "  \"To plot  a measure of a classifier's performance\": 0,\n",
       "  'The more the Area under the ROC curve, the better will be the model performance.The model with more AUROC can be selected.': 1,\n",
       "  '0 to 1, a model whose predictions are 100% wrong has AUC 0.0, one whose predictions are 100% correct AUC is 1.0': 1,\n",
       "  'A higher curve ': 1,\n",
       "  'A model that is more accurate will have a curve that hugs the top left corner. Thus, the closer the AUROC is to 1 the better the classifier. ': 1,\n",
       "  '1': 0,\n",
       "  'We can plot 1 ROC curve for each classifier and calculate the AUC for each classifier and compare to find the best classifier': 1,\n",
       "  'a': 0,\n",
       "  'Plot FP versus TP, more the area under the curve is the better model.': 1,\n",
       "  '.plot FP vs TP.  which classifier gives more area that will be better model': 1,\n",
       "  'plot FP versus TP. classifier having more area under the curve will be better model': 1,\n",
       "  'More the AUROC plot, the better the model is': 1,\n",
       "  'It is plotted by using one class versus the rest of the classes, TP': 0,\n",
       "  'it can classify upto 3 classes': 0,\n",
       "  'used to identify the best classifier from a group of models by comparing their respective areas under the curve. The classifier having the highest AUC being considered the most effective in distinguishing between the positive and negative classes.': 1,\n",
       "  'The area lying under the AUROC is used to identify the best classifier from a multitude of models': 1,\n",
       "  'We can select the best classifier with highest area under the curve.': 1,\n",
       "  'AUROC': 0,\n",
       "  'They are: 1) True Positive Rate (TPR) ; 2) False Positive Rate (FPR)': 0,\n",
       "  'Area under the curve is best used in binary classification problems.': 0,\n",
       "  'ROC is plotted with one class versus other in the multiclass problems , There are same number of roc curves as much as the no of classes, so for multi class multiples roc curves will be used. ROC  score for each class can be calculated an dbest is chosen': 0,\n",
       "  'when it is highest area under the curve.': 1,\n",
       "  'roc curve should be to upper left corner of the graph, then it will identify best classifier.': 1,\n",
       "  'ROC curves can be plotted with the methodology of using one class versus the rest. Use this one-versus-rest for each class and we will have the same number of curves as classes. The AUC score can also be calculated for each class individually. The area under the ROC curve (AUC) results were considered excellent for AUC values between 0.9-1': 1},\n",
       " 'What is the mean of an attribute that is z-score normalized?': {'0': 1,\n",
       "  'The mean is 0.': 1,\n",
       "  'zero': 1,\n",
       "  'zero, 0': 1,\n",
       "  '1': 0,\n",
       "  'Zero': 1,\n",
       "  'here the values will be normalized based on the mean and standard deviation of the data': 0,\n",
       "  'it would be zero ': 1,\n",
       "  'mean=0': 1,\n",
       "  'zero ': 1,\n",
       "  'values are normalized in dataset': 0,\n",
       "  'Mean = 0 ': 1,\n",
       "  'the mean is zero': 1,\n",
       "  'zero. it is process of normalizing each value in data set': 1,\n",
       "  'the mean is 0': 1,\n",
       "  'mean is 0': 1,\n",
       "  '0 is the mean of an attribute that is Z-score normalized.': 1,\n",
       "  'mean is 0.': 1,\n",
       "  'Zero(0)': 1},\n",
       " 'How can you fill the missing values for a categorical attribute?': {'one hot encoding or label encoder': 0,\n",
       "  'by grouping data of other attribute in similar range and applying the most common category': 1,\n",
       "  'We can apply grouping based on similar attributes and filling the missing values accordingly.': 1,\n",
       "  'if possible i will igrone if not then i make mode that predicte values for exampe like desicion tree etc like that if data set is large i will try to delet those columns': 1,\n",
       "  'k-means': 0,\n",
       "  'mode': 1,\n",
       "  'By building a model that predicts the missing values after getting replaced with similar types of averages.': 0,\n",
       "  'Ignore less necessary values, use the most frequent categorical value it takes': 1,\n",
       "  'grouping by other attributes': 0,\n",
       "  'missing values for categorical attribute can be filled using mode ': 1,\n",
       "  'median, mode, using KNN on the dataset with the missing value to be classified.': 0,\n",
       "  'i f we try to one hot encode them ,we  will consider each unique value as column similar way consider all empty values as sepearte column': 0,\n",
       "  'fill it with another class, or ignore the records if the records having missing values are less': 0,\n",
       "  'using regression to predict the category': 0,\n",
       "  'replace missing values by most common class or use knn to find the missing value': 0,\n",
       "  'Build model to predict missing values': 0,\n",
       "  't': 0,\n",
       "  'By using the rest of the data to find the missing values or by using dummies': 0,\n",
       "  'we can fill the missing values of categorical attribute by using the mode of the corresponding attribute': 1,\n",
       "  'replace with most frequent value or use knn model or any other model to predict that missing one. decision trees are good at filling missing data.': 1,\n",
       "  'Using mean, median and other methods': 0,\n",
       "  'by using dummies': 0,\n",
       "  'replace them with the most common or occurring class': 1,\n",
       "  'fill it with the most frequent value in the column': 1,\n",
       "  'treat missing data as new category, design a machine learning model to predict missing values': 0,\n",
       "  'by using the fillers and mean': 0,\n",
       "  'Implement one hot encoding and fill in the values by taking the frequency (mode) of each class': 0,\n",
       "  'using dummies ': 0,\n",
       "  'Fill with NaNS': 0,\n",
       "  'replace it with the value that is most frequent ': 1,\n",
       "  'Filling with most occurring class or use decision tree or regression analysis.': 0,\n",
       "  'Dummies': 0,\n",
       "  'we can fill the missing values by most frequent values in the data or using KNN nearest neighbours': 1,\n",
       "  'fillna()': 0,\n",
       "  \"we'll some times fill it with most occuring value or sometimes calculate the value using other features, if they form any relation \": 1,\n",
       "  'we will fill it with attribute mean or will fill based on other dependent features.': 0,\n",
       "  'fill in the most frequent value': 1,\n",
       "  'Treating as another category': 0,\n",
       "  'We have a high co-relation between the variables for some reason , caluculate the regression analysis. we can caluculate the missing values': 0,\n",
       "  'using mean or median': 0,\n",
       "  'ignoring variable,when it is not significant': 0,\n",
       "  'We can fill all missing values with another category': 0,\n",
       "  'frequent vlaues': 1,\n",
       "  'Creating new model to predict the missing balues': 0,\n",
       "  'fill with duplicate': 0,\n",
       "  'some duplicate': 0,\n",
       "  'we want to ignore variable ,if it is not important, we can use imputer to fill those missing values.': 0,\n",
       "  'a': 0,\n",
       "  'Ignore observations of missing values if we are dealing with large data sets and less number of records has missing values': 0,\n",
       "  'replace with average': 0,\n",
       "  'mean': 0,\n",
       "  'we can replace the missing values with the most frequent value or we can simply use the dummies ': 1,\n",
       "  'If the dataset was large then I will remove some rows of missing values or I will replace with the most frequent value occured in dataset': 1,\n",
       "  'Eliminate the observation, Replace with frequent value': 1,\n",
       "  ' Simply replacing the missing value  with the average value of the data': 0,\n",
       "  'Fill with most occurred value or fill with duplicates or NaNs': 1,\n",
       "  'dummies': 0,\n",
       "  'we can fill missing value by replacing the missed values with the high values which are more frequent': 1,\n",
       "  'we can use the function df = df.fillna(df.mode().iloc[0]) to fill the missing values.': 0,\n",
       "  'removing variable of missing value': 0,\n",
       "  'Deleting observations': 0,\n",
       "  'DUMMIES': 0,\n",
       "  'we can use fillna() methods ; dummies.': 0,\n",
       "  'we can ignore the varibles,   or replace it by a frequent value': 1,\n",
       "  'to develop a model to predict a value': 0,\n",
       "  '1. if we have a large data sets and less number of records has missing values then we can ignore the missing values 2. developing a model to find the missing values ': 0},\n",
       " 'Why do the sinusoids and co-sinusoids form a bases set for time series analysis?': {'because they decompose each other in nature': 0,\n",
       "  'sin(a .(theta)) . cos(b.(theta))=0 for all a,b . So each sinoside and co sinosuide +ve and -Ve values cancel each other. THereby their dot prod is 0 and they are orthogonal and form basis set': 2,\n",
       "  'sinusoids and co-sinusoids are orthogonal to each other so they span the entire space and forms a bases set ': 2,\n",
       "  'form a bases to ': 0,\n",
       "  'Time series data is typically depicted via wave signals and can be decomposed into a form of sine and cosine waves. This is because they are orthogonal and independent to each other.': 2,\n",
       "  'Sinusoids and Co-Sinusoids are orthogonal to each other which allows them to form a bases set for time series analysis. We can span the same over an infinite number of these bases which is ideal for time series analysis.': 2,\n",
       "  'They form the basis as any time series data can be represented as the combination of sinusoids and cosinusoids.': 1,\n",
       "  'd': 0,\n",
       "  'sinusoids and co-sinusoids are orthogonal to each other, hence they form a bases vector.': 2,\n",
       "  'As they cancel each other out when we map them onto a graph.': 0,\n",
       "  'because they are independent, orthogonal and can be used to represent any other periodic signal. ': 2,\n",
       "  'when sinusoids and co-sinusoids are convoluted against each other or when a point to point mutiplication is performed we observe that they cancel out each other hence giving us a value of zero, which gives us the information that they are orthogonal to each other and as they span the entire vector space we can say that they form a bases set': 2,\n",
       "  '-': 0,\n",
       "  'they are mutually orthogonal and collection of these waves can be used to draw any given time series function in same space hence they form a basis set': 2,\n",
       "  'Any data collected over a period of time can be described with a combination of sines and cosines thus they form a basis set for time series data ': 0,\n",
       "  'Because these functions are periodic.': 0,\n",
       "  'because the dot product between sinusoids and co-sinusoids curves is zero': 2,\n",
       "  ' ': 0,\n",
       "  'no': 0,\n",
       "  'Because they are orthogonal ': 2,\n",
       "  'Because they are orthogonal': 2,\n",
       "  'Due to frequency': 0,\n",
       "  'Because they are orthogonal to each other and span the entire space as they are periodic in nature.  (sin(awt) and cos(bwt) are orthogonal for any a,b value).': 2,\n",
       "  'm': 0,\n",
       "  'as they are orthogonal to each other.': 2,\n",
       "  'because according to FT, any wave can be represented as a combination of the sin waves and cosine waves of different w(omega) values this is because sin and cosine of any w(omega) value are orthogonal to each other. And all the waves can be decomposed into its constituent sin and cosine waves of different w(omegas) using the FT.': 2,\n",
       "  'The sinusoids and co-sinusoids form a bases set for time series analysis because the sin and cosine functions are perpendicular and orthogonal to each other.': 2,\n",
       "  'they form bases set for time series analysis because of combination of both sinusoids and co-sinusoids together will make it like that ': 0,\n",
       "  'none': 0,\n",
       "  ' it has variety wide high inherited time': 0,\n",
       "  'Because when they are independent to each other and orthogonal and span entire vector space hence form the bases set.': 2,\n",
       "  'both are orthogonal to each other.ie cos(alpha*t) and sin(beta*t) are perpendicular to each other for all alpha,beta.so both sin and cos form ': 2,\n",
       "  'because the sine wave and co-sine wave are good in predicting the time series, thus forming the same base.': 0,\n",
       "  'sinusoids': 0,\n",
       "  'using the combination of sine and cosine, we can get all other trigonometric functions (like tan, cot, sec, cosec). Hence sin and cos span the total space used in time series analysis.': 1,\n",
       "  'because asinx+bcosx=0, if a1!=b.': 1,\n",
       "  'easy to understand': 0,\n",
       "  'ssince time key can be represented as sum of sinusoids and osilation frequenchy': 0,\n",
       "  'Sinusoids and Co-sinusoids are orthogonal. So that is why they will perform basis of (sinwt.coswt) = 0': 2,\n",
       "  'fourier decomposition ': 0,\n",
       "  'because they are orthogonal': 2,\n",
       "  \"sinusoids and co-sinusoids are always orthogonal. So, that's why they'll form basis set. integral(sinawt.cosbwt) = 0 for all a and b \": 2,\n",
       "  'because sinosoids and cosinu sides are in series ': 0,\n",
       "  'because functions are periodical so periodicity is, time series analysis': 0,\n",
       "  'because it has high frequency': 0,\n",
       "  'sinusoids  form by frequencies and correlation and co-sinusoids are form by': 0,\n",
       "  'Sinusoids and Co-Sinusoids form bases set ': 0,\n",
       "  'sinusoids and co sinusoids form bases because': 0,\n",
       "  \"Don't know sir\": 0,\n",
       "  'The sinusoids and co-sinusoids form a bases set for time series analysis as the dot product tends to zero.': 2,\n",
       "  'sinusoids and co-sinusoids form a bases set for time series analysis because ': 0,\n",
       "  'it is very easy to understand': 0,\n",
       "  'sinusooids': 0,\n",
       "  'ortogonal': 1,\n",
       "  'because of Fourier Decomposition': 0,\n",
       "  '..': 0,\n",
       "  '.': 0,\n",
       "  'sinusoidal  and co-sinusodial are always orthogonal': 2,\n",
       "  ' it retains the wave shape when added to another sine wave of the same frequency ': 0,\n",
       "  'these functions are periodical, perodicity is time series analysis': 0,\n",
       "  'g': 0,\n",
       "  'fourerdra transformation': 0,\n",
       "  'Because they are all orthogonal to each other': 2,\n",
       "  'N': 0},\n",
       " 'What is Bagging?': {'bagging means we use this concept in random forest it combination of different ensembles in that different models will be present  we aggregate the final output of different models': 1,\n",
       "  'Bagging refers to the type of aggregate classifier where each value from model is collected and the final value is decided by a voting system': 1,\n",
       "  'bagging is one of the method in ensemble algorithms. we create random subsets from data (ex: n random subsets from data) and we create n classifiers. we assign each dataset to each classifier and we train all classifiers. for test dataset we will get the output based on the majority votes. bagging is used to reduce variance': 1,\n",
       "  'bagging is ensembling the variant types   ': 0,\n",
       "  'Bagging is an of ensemble method with multiple classifiers used together for improving accuracy and reducing noise': 1,\n",
       "  'In Bagging, we make use of multiple models over mini-batches rather than a single one and make an aggregate prediction. Thus, resulting in a reduction in the error we get.': 1,\n",
       "  'Bagging is a technique where same model is used multiples times with differrent parameters, on batches of data. It is a type of ensemble modelling.': 1,\n",
       "  'bagging is used in decion trees to reduce the variance .we divide the data into random sub sets to use multiple classifiers on them': 0,\n",
       "  'It is an ensemble classifier used to reduce variance (bring to a range) of dataset with outliers by appending additional data in the training phase.. ': 1,\n",
       "  'A method that divides the data and add them into separate bags.': 0,\n",
       "  'same model, with same hyperparameters is trained with different subsets of data and result is averaged. ': 1,\n",
       "  'Bagging is the process of a taking a neural network with different architecture to train on a set of data after which the best among the architectures are chosen by using a voting mechanism': 1,\n",
       "  '-': 0,\n",
       "  'bagging - bootstrap aggregator, is an ensemble classifier that trains multiple classifiers on random samples of training data.': 1,\n",
       "  'Bagging is a regularization technique that comprises of a collection of models with different hyperparameters which will be trained on the same data or different batches of the data  ': 1,\n",
       "  'it  is a ensemble learning technique which is used to reduce variance. In this technique data sample is chosen randomly and that is selected with replacement': 0,\n",
       "  'Bagging is a bootstrap aggregation method used for predicting the output, based upon majority.In which we pass different samples of data to models with different hyper parameters and get outputs ,we can conclude the output by majority. ': 1,\n",
       "  'It helps in reducing variance in noisy data.': 0,\n",
       "  'bagging is used to reduce variance in a noiesey  dataset': 0,\n",
       "  'bagging is a supervised learning and learns from the independent and parallel class': 0,\n",
       "  'Dataset is subdivided into multiple datasets and we train each sub dataset separately.': 1,\n",
       "  'Bagging refers to the classification or segregation of similar class values.': 0,\n",
       "  'Bagging is a process of grouping various classes in a dataset into multiple different clusters.': 0,\n",
       "  'Bagging  is an ensemble learning technique used to decrease noise caused by variance  (bootstrap  aggregation)': 1,\n",
       "  'Bagging is used to improve the accuracy of machine learning algorithms': 0,\n",
       "  'Bootstrap Aggregation is when the input is given to the same classifiers with different thresholds that generate various outputs, and the most accurate output is chosen.': 1,\n",
       "  'which is used to reduce variance in a noisy data ': 0,\n",
       "  'it is the process that is used to reduce the variance and helps increase the accuracy of a classification problem': 0,\n",
       "  'Using multiple, models with small variations in the training data(subsets), variation in hyper parameters etc. and performing voting mechanism to create robust models.': 1,\n",
       "  'Bagging is ensemble learning method used to reduce the noise in dataset and is also known as bootstrap aggregation.': 1,\n",
       "  'Bagging is a ensemble method which is used In a noisy dataset, to reduce a variance.': 1,\n",
       "  'its reduce the variance': 0,\n",
       "  'introducing varience': 0,\n",
       "  'it is also known as bootstrap aggregrating helps to improve the performance and accuracy of machine learning algorithms': 1,\n",
       "  'Bagging is about the feature sampling(i.e if we have dataset try to take some random samples from the dataset and give to each model)': 1,\n",
       "  'it is an ensemble method that uses bootstrap aggregation and replicates of the original training data to fit predictive models': 1,\n",
       "  'making a cluster of data points to predict the values is called bagging.': 0,\n",
       "  'bagging is the ensemble learning method that is commonly used to reduce variance within a noisy dataset': 1,\n",
       "  'combining 2 or more same classifiers/models/Neural-networks with different architecture (e.g: model1 = knn with k =9 ; model2 = knn with k = 5 ....) to get an more accurate prediction': 1,\n",
       "  'just like random forest it basically taking all models and giving sample of the original dataset to each model and predicting the output. among these which has occured more is taken as the final output.': 1,\n",
       "  'Bagging is used to reduce variance within a noisy dataset.': 0,\n",
       "  'bagging is nothing but tooll used to reduce variance in a data set ': 0,\n",
       "  'In bagging we take a dataset and performes many architectures and in that we can take the best accuracy model, this process is bagging.': 1,\n",
       "  'it is used in bootstrap algorithm , it is a ensemble method used to improve stability and accuracy by reducing the variance of a noisy dataset.': 1,\n",
       "  'it technique leads to reduce noice ': 0,\n",
       "  \"In bagging, we employ k number of classifiers which are of same type and we implement them with different parameters and we'll choose the class which was predicted max times like KNN or we can do voting. For regression we can take average.\": 1,\n",
       "  'Bagging refers to which stores or contains independent variables,': 0,\n",
       "  'bagging is a bootstrap algoritham used for an': 1,\n",
       "  'Bagging is a technique which reduces variance by adding data': 0,\n",
       "  'it  is used to reduce the variance in data': 0,\n",
       "  'Bagging is a technique used to reduce variance with noisy dataset': 1,\n",
       "  \"it's also called as bootstrap aggregation which is basically used to reduce variance in the data which contains noise.\": 1,\n",
       "  'Bagging is used to reduce the variance': 0,\n",
       "  'A method which is used to reduce variance in a dataset which is not preprocessed(noisy dataset)': 0,\n",
       "  'Bagging is process of is the ensemble learning method through which variance is reduces from a very raw dataset. Also called as bootstrap aggregation.': 1,\n",
       "  ' Bagging is a technique used to reduce variance with noisy dataset.': 1,\n",
       "  'bagging is a learning method that is commonly  used to reduce variance within a noisy database': 0,\n",
       "  'bagging is an algorithm that reduces variance within a dataset.': 0,\n",
       "  'it is used to reduce the voice ': 0,\n",
       "  'it creates a different training subset from sample traininng data with replacement and the final output will be based on majority voting. it combines bootstrapping and aggregation': 1,\n",
       "  'its a technique to reduce the noise in the data': 0,\n",
       "  '.': 0,\n",
       "  'bagging is the method used for reducing variance, helps to improve accuracy of ML algorithms': 0,\n",
       "  'an algorithm used to reduce variance ,likely we can say that it (avoids the overfitting).': 0,\n",
       "  'it is used  to reduce variance here data smaples choosen ramdomly and it selected is replacement': 0,\n",
       "  'bag': 0,\n",
       "  'Bagging is the ensemble learning method that is commonly used to reduce variance within a noisy dataset.': 1,\n",
       "  'It was an ensemble technique, in which we will use the training dataset and split that into sample of random groups of data and model them. It helps to reduce the variance in an large inconsistent data.': 1,\n",
       "  'used to improve the performance and accuracy': 0,\n",
       "  'Bagging uses a simple approach that do again and again thus improve the estimate of one by combining the estimates of many': 0},\n",
       " 'How do you determine the class value from an ensemble classifier?': {'it depends on the situation': 0,\n",
       "  'By voting among the values generated by each models': 1,\n",
       "  'we use voting method . we get outputs from all classifiers based on that we try to find out majority class. that majority class is the output of ensemble classifier': 1,\n",
       "  'none': 0,\n",
       "  'By a form of voting taken from the outputs of each of the classifiers': 1,\n",
       "  'Each classifier votes for a class that it predicts to be true. The class with the maximum votes is chosen as the final prediction.': 1,\n",
       "  'The class value from an ensemble classifier can be identified by taking the majority of class predicted.': 1,\n",
       "  'based on the classifier we used for that sub data set we may use entropy ,  gini index': 0,\n",
       "  'This classifier uses random feature selection to determine the next split, at each node.': 0,\n",
       "  \"Don't know\": 0,\n",
       "  'by voting amongst all the individual results': 1,\n",
       "  'The class value from an ensemble classifier is chosen by the method of voting': 1,\n",
       "  '-': 0,\n",
       "  'voting on class value and taking majority.': 1,\n",
       "  'The class value from an ensemble classifier can be determined based on the majority vote of predictions made all the models part of the ensemble classifier ': 1,\n",
       "  'we can determine by its popularity. Return the class which occurs more times.': 1,\n",
       "  'by taking majority from the output of the models.': 1,\n",
       "  'By some selection criteria like information gain, the attributes are selected and an ensemble is constructed. The last element of ensemble gives class value.': 0,\n",
       "  'no': 0,\n",
       "  'Perform voting to determine the class value. The class with maximum voting is considered as output.': 1,\n",
       "  'By randomly selection of attributes': 0,\n",
       "  'By a voting technique (choosing most accurate output) or combining all the results and taking the average.': 1,\n",
       "  'm': 0,\n",
       "  'we determine the class value by finding the mean': 0,\n",
       "  'We use a voting mechanism to get the majority from all the models in the ensemble.': 1,\n",
       "  'we can determine the class value from an ensemble classifier by ': 0,\n",
       "  'as various trees parrallely train and predict on different aspects , the majoirity among the all classifier will be considered as class value ': 1,\n",
       "  'it is generated using a random selection of attributes at each node to determine the split': 0,\n",
       "  'Majority of the class value ': 1,\n",
       "  'by taking all the outputs and considering the most occuring value (voting)': 1,\n",
       "  'by using ensemble classifier.': 0,\n",
       "  'ensemble': 0,\n",
       "  'by using \"VOTING\" among the outputs of different classifiers in that ensemble': 1,\n",
       "  'the class value is determined by taking all outputs from all the predicted models and taking the value which has occured more number of times.': 1,\n",
       "  'bagging': 0,\n",
       "  'using bagging and bagging regression': 0,\n",
       "  'we can choose by dominant class which which was predicted by classifiers like knn ': 1,\n",
       "  'we can use bagging or random forest to ': 0,\n",
       "  'dont know': 0,\n",
       "  'We can determine class values by choosing dominant class which was predicted by more classifiers. Like KNN we choose which was the dominant class.': 1,\n",
       "  'by ': 0,\n",
       "  'we can determine by its popularity': 1,\n",
       "  'by using random variables in the attributes': 0,\n",
       "  'to determine': 0,\n",
       "  'in this method we randomly select a variable at each node and that determines the split,the class value is determined by taking all outputs from all the predicted models and taking the value which has occured more number of times.': 1,\n",
       "  'by random selection of nodes': 0,\n",
       "  'We can determine the class value by using random attributes ': 0,\n",
       "  'voting is used': 1,\n",
       "  'to determine the class value from an ensemble classifier  we want to predict ': 0,\n",
       "  'Every classifier in the ensemble  is generated using a random selection of attributes at each node to find the split. During classification, each tree votes and the most voted class is selected': 1,\n",
       "  'ensemble classifier generally uses random selection of attributes at each node.': 0,\n",
       "  '.': 0,\n",
       "  'we can determine class values by choosing dominant class': 1,\n",
       "  'better performance': 0,\n",
       "  'based on its popularity': 0,\n",
       "  'at first they will determine the class value and': 0,\n",
       "  'we will determine the class value for an ensemble classifier using the number of sample groups of data in a decision tree.': 0,\n",
       "  '.generated using a random selection of attributes at each node to determine the split': 0,\n",
       "  'Each classifier in the ensemble is a decision tree classifier and is generated using a random selection of attributes at each node to determine the split. During classification, each tree votes and the most popular class is returned': 1},\n",
       " 'When do two independent vectors become orthogonal?': {'The dot product is zero.': 1,\n",
       "  'dot product of two vectors becomes zero': 1,\n",
       "  'they cannot be orthogonal': 0,\n",
       "  'perpendicular to each other': 0,\n",
       "  'dot product of the vectors should be zero': 1,\n",
       "  'When one cannot be represented as a scalar modification of the latter; ie perpendicular; ie dot product of zero': 1,\n",
       "  'when the dot product of the two vectors equals 0': 1,\n",
       "  ' their dot product is 0': 1,\n",
       "  'dot product=0': 1,\n",
       "  'dot product between the vectors is equal to zero.': 1,\n",
       "  'when their dot product is 0': 1,\n",
       "  'never': 0,\n",
       "  'when the dot product of vectors is equal to zero.': 1,\n",
       "  'when the dot product is 0': 1,\n",
       "  'when they are perpendicular to each other': 0,\n",
       "  'when the dot product of the two vectors is zero ': 1,\n",
       "  'When the angle between the two vectors is 90 degrees -> the dot product = 0.': 1,\n",
       "  'When Dot product of two vectors is equal to  zero.': 1,\n",
       "  'if they are pendicular to each other means the dot product of two vectors equals zero.': 1,\n",
       "  'when the dot product of two vectors is zero.': 1,\n",
       "  'If the vectors make 90 degree between them': 0,\n",
       "  'Dot Product of the two vectors is zero. (Same as vectors being perpendicular)': 1,\n",
       "  'if the dot product of the independent vectors is zero, then the vectors are orthogonal to each other': 1,\n",
       "  'We say that the two vectors are orthogonal if they are perpendicular to each other.  i.e if the projection is falling on the other vector and if that projection makes a right angled triangle, then we can say that these are orthogonal.': 0,\n",
       "  'dot product is zero.': 1,\n",
       "  'when dot product of the vectors is zero ': 1,\n",
       "  'If dot product is zero it is orthogonal': 1,\n",
       "  'when the dot product of given vectors is zero': 1,\n",
       "  'dot product of two vectors is zero': 1,\n",
       "  'They are perpendicular or their dot product is zero': 1,\n",
       "  'If the vectors are perpendicular to each other': 0,\n",
       "  'When they are perpendicular, or their dot product is equal to 0': 1,\n",
       "  'If their dot product is zero': 1,\n",
       "  ' dot product of 2  vectors equals 0': 1,\n",
       "  'dot product of two vectors is zero.': 1,\n",
       "  'Dot product = 0': 1,\n",
       "  'Dot product of 2 vectors becomes 0': 1,\n",
       "  'when their Dot product is zero': 1,\n",
       "  'Perpendicular to each other (dot product = 0)': 1,\n",
       "  'when their dot product is equal to zero': 1,\n",
       "  'dot product of vectors should become 0': 1,\n",
       "  'when their dot product is equal to zero(perpendicular)': 1,\n",
       "  'when their dot product is zero': 1,\n",
       "  ' vectors are perpendicular to each other': 0,\n",
       "  'If the Two vectors are perpendicular to each other.': 0,\n",
       "  'when angle 90 degrees which are perpendicular to each other': 0,\n",
       "  'Dot product of two independent vecors is Zero': 1,\n",
       "  'When they are perpendicular or also their dot product should be equal to zero': 1,\n",
       "  'dot product = 0.': 1,\n",
       "  'When their dot product is 0': 1,\n",
       "  'when the two vectors are perpendicular to each other': 0,\n",
       "  'Dot product of the given vectors is zero': 1,\n",
       "  'when the angle between vectors is 90 degrees': 0,\n",
       "  'when they are perpendicular': 0,\n",
       "  'when the vectors are perpendicular to each other or when the vectors dot product results to zero': 1,\n",
       "  'when dot product of those two vectors is zero,then they become orthogonal and they form a basis': 1,\n",
       "  'dot product = zero (0) (also know as perpendicular)': 1,\n",
       "  'dot product is zero': 1,\n",
       "  'independent  vectors become orthogonal if they are perpendicular to each other. the dot product of the two vectors is zero.': 1,\n",
       "  'when their dot product=0 or when the 2 vecotrs are perpendicular to each other': 1,\n",
       "  'when dot product of given vectors = 0': 1,\n",
       "  'perpendicular to each other or dot product is zero.': 1,\n",
       "  'a.b=0': 1,\n",
       "  \"only If the dot product of 2 vectors is '0'.\": 1,\n",
       "  'when their dot product is 0 and they both are perpendicular': 1,\n",
       "  'If the two independent vectors are perpendicular to eachother. Dot product of vector is 0.': 1,\n",
       "  'when the dot product of  two vectors equals zero': 1,\n",
       "  'when they are perpendicular or dot product btwn them is zero': 1,\n",
       "  'when dot product is zero': 1,\n",
       "  'dot product of vectors is zero': 1,\n",
       "  'dot product will be zero': 1,\n",
       "  'dot product is equal to zero': 1,\n",
       "  'if they are perpendicular to each other': 0,\n",
       "  'if the dot product of them is zero then they are orthogonal': 1,\n",
       "  'Dot product of two vectors is zero ( cos90 =0)': 1,\n",
       "  'when doot product equal to zero': 1,\n",
       "  'dot product of two vectors is equals to zero': 1,\n",
       "  'the dot product of the two vectors is zero.': 1,\n",
       "  'the dot product of two vectors is zero': 1,\n",
       "  'If The given two vectors are perpendicular to each other.': 0,\n",
       "  'If the dot product of the two vectors is zero or if the two vectors are perpendicular to each other.': 1,\n",
       "  'if every vector has magnitude 1 and the set of vectors are mutually orthogonal': 0,\n",
       "  '2 independent vectors are orthogonal if they are perpendicular to each other. the dot product of the two vectors is zero.': 1,\n",
       "  'two independent vectors become orthogonal when dot product is zero': 1,\n",
       "  'If they are perpendicular to each other': 0,\n",
       "  'dot product of the two vectors is zero': 1,\n",
       "  'if they are perpendicular to each other(dot product=0)': 1,\n",
       "  'if they are perepndicular to each other,that is their dot product is zero': 1,\n",
       "  'if the dot product is equal to 0': 1,\n",
       "  'if they are perpendicular to each other. i.e. the dot product of the two vectors is zero.': 1,\n",
       "  'if the dotproduct equals to 0': 1,\n",
       "  'When there dot product is 0 or angle between them is 90 degree': 1,\n",
       "  'if dot product of the two vectors is zero': 1,\n",
       "  ' the dot product is zero': 1,\n",
       "  'when they are perpendicular to each other which can be checked if the dot product of the two vectors is zero.': 1,\n",
       "  'when the dot product of those 2 vectors is zero ': 1,\n",
       "  'BEACAUSE THE DOT PRODUCT OF TWO VECTORS ARE ZERO': 1,\n",
       "  'When they are perpendicular to each other, i.e their dot product = 0 coz cos90 degrees is = 0': 1,\n",
       "  'they are perpendicular to each other or dot product of vectors are zero': 1,\n",
       "  'Dot product of the two vectors  = 0 ,ie, they are perpendicular.': 1,\n",
       "  'When dot product of the two vectors is zero and they are perpendicular': 1,\n",
       "  'when dot product of two vectors is Zero(0).': 1,\n",
       "  'dot product = 0': 1,\n",
       "  'dot product zero': 1,\n",
       "  'when dot product is equal to 0': 1,\n",
       "  'If they are perpendicular to each other ,the dot product of the two vectors is zero': 1,\n",
       "  'When two vectors are perpendicular.': 0,\n",
       "  'when they are perpendicular to each other and dot product of vectors is 0': 1,\n",
       "  ' Two independent vectors become orthogonal when their dot product is ZERO': 1},\n",
       " 'You are provided the height\\xa0data\\xa0from all boys and girls in your class. After the data is collected, you observe that height information for some students is missing. How would you fill the missin...': {'Ignoring the tuple.': 0,\n",
       "  'by the mean height seperately for girls and boys': 1,\n",
       "  'using mean of the collected data': 1,\n",
       "  'mean of heights': 1,\n",
       "  'by taking the average value of the recorded heights': 1,\n",
       "  'Remove the height feature if not a crucial feature, else remove the records of students with missing heights': 0,\n",
       "  'Use the mean values of height of boys and girls respectively': 1,\n",
       "  'mean': 1,\n",
       "  'fill automatically with global constants, manually or by ignoring the tuple.': 0,\n",
       "  'we could fiil in the mean or median in the missing places.': 1,\n",
       "  'i would fill the data with mean of the heights of the total class': 1,\n",
       "  'I would fill the missing data with the mean height ': 1,\n",
       "  'global constant,mean or mode': 1,\n",
       "  'with the mean of the data': 1,\n",
       "  'we take the mean of the existing values and fill this mean to the empty places': 1,\n",
       "  'By filling them with mean value of heights': 1,\n",
       "  '1). ignore the tuple 2). Automatic fill 3). Manual fill': 0,\n",
       "  'Mean value of heights': 1,\n",
       "  '1 will fill those missing values with mean value of the height.': 1,\n",
       "  'I would fill those missing values with mean value of the heights. ': 1,\n",
       "  'X = (mean, interpollate, median, mode ).  applied on the class of the data i.e., X(all boys) if missing data is of boy etc.': 1,\n",
       "  'Sample Mean of the same class.': 1,\n",
       "  'by taking the average of the heights': 1,\n",
       "  'Find the average of the heights and replace it in the missing places': 1,\n",
       "  'get attribute mean for samples belonging to same class(girls/boys)': 1,\n",
       "  'The missing data can be filled using the class mean ': 1,\n",
       "  'we can use interpolate and fill it with mean values of data . ': 1,\n",
       "  'I will fill the missing data with mean height': 1,\n",
       "  'Mean': 1,\n",
       "  'Mean of the available values': 1,\n",
       "  'Fill it with NaN ': 0,\n",
       "  \"I would take the mean of all students of the category 'boys' and fill in\": 1,\n",
       "  'i will fill mean of the column or manulay assingn by if condtion if some value for that above one is more fill this or that': 1,\n",
       "  'Mean of all values belonging to same class': 1,\n",
       "  'with mean , interpolation, mode, previous value or  next value ': 1,\n",
       "  'mean height': 1,\n",
       "  'Find mean of boys and girls seperately and fill missing values': 1,\n",
       "  'Mean of the heights': 1,\n",
       "  'By overall mean, sample mean or mode.': 1,\n",
       "  ' attribute  mean  of the class': 1,\n",
       "  'with the attribute mean of samples that belong to the same class. Here, the classes are boys and girls.': 1,\n",
       "  'I would fill the missing data of boys by calculating mean of their heights and similarly calculate mean of heights of girls to fill in their missing data': 1,\n",
       "  'i will fill them with the mean of the available data': 1,\n",
       "  'fillna function': 0,\n",
       "  'Using fillna(), we can fill NaN for the missing values alsoWe can also use Mean , Median, Mode way also': 1,\n",
       "  'filling with mean of non-missing students': 1,\n",
       "  'mean height of class': 1,\n",
       "  'i will fill the missing value with the mode of given values': 1,\n",
       "  'attribute mean of heights or global constant \"unknown\".': 1,\n",
       "  'Fill it by using mean,median or mode': 1,\n",
       "  'using the mean of the data': 1,\n",
       "  'droping the data fro data set or filling with mean or mode': 1,\n",
       "  \"mode or median of the all student's height \": 1,\n",
       "  'fill nan,na': 0,\n",
       "  'we find the mean of the data and fill the missing values with the mean': 1,\n",
       "  'we can calculate mean,mode and global constant and use fillna() method from pandas to fill the missing values': 1,\n",
       "  'global constant , mode , attribute mean , attribute mean for all samples belonging to the same': 1,\n",
       "  'fill in the missing value manually': 0,\n",
       "  'I would fill height of the boys with mean height of boys( mean of height of boys available) . for girls height with mean height of girls': 1,\n",
       "  'attribute mean of all samples of that class.': 1,\n",
       "  'either fill the data manually or automatically by using a global constant(\"unkown\") or ingore the tuple': 0,\n",
       "  'i will fill the missing value with mean ': 1,\n",
       "  ' By frequency disribution': 0,\n",
       "  'mean or mode': 1,\n",
       "  'nbhb': 0,\n",
       "  'fill with the attribute mean': 1,\n",
       "  'we can enter mean value of height': 1,\n",
       "  'For the missing height of a boy, we find the average height of all the boys and fill it in the missing cells. Similarly average height of girls is found and filled in missing cells accordingly.': 1,\n",
       "  'yes': 0,\n",
       "  'I will fill the missing values by Attribute mean of heights': 1,\n",
       "  'Fill it with the mean of height data': 1,\n",
       "  'mean of height': 1,\n",
       "  'mean of the class': 1,\n",
       "  'attribute mean,mode': 1,\n",
       "  'by using the mean value of the height of the boys we fill the missing data spaces': 1,\n",
       "  'We can find mean of given observation and fill it missing values. ': 1,\n",
       "  'global constant, mean or mode': 1,\n",
       "  'no': 0,\n",
       "  'by using mean ': 1,\n",
       "  'First fill by Nan(not a number) and then fill by fillna ': 0,\n",
       "  'By doing the average mean or find the mode of that class.': 1,\n",
       "  'no ': 0,\n",
       "  'Global constant,mean or mode': 1,\n",
       "  'ddd': 0,\n",
       "  'we can fill it with the mean value': 1,\n",
       "  '.': 0,\n",
       "  'dont know': 0,\n",
       "  'by using mean or based on class mean or like we can do with frequency too in some cases': 1,\n",
       "  'we calculate mean and fill the missing values with mean': 1,\n",
       "  's': 0,\n",
       "  'replace it with mean,or mode ': 1,\n",
       "  'absolute mean': 1,\n",
       "  'data handling': 0,\n",
       "  'fill it automatically with the attribute mean of the that class or with the mode.': 1,\n",
       "  'i will use interpolation of each column or i will fill them witht the most frequesnt value of each column and assign them those values ': 1,\n",
       "  'MEAN': 1,\n",
       "  'I would basically take average or randomize the numbers from the lowest to the largest height obtained': 1,\n",
       "  'by using missing data mean': 0,\n",
       "  'interpolation method from python or forward fill or backwardfill depending on the gender': 1,\n",
       "  'global constant,mode or mean': 1,\n",
       "  'I will find the median or mean or mode of the heights and replace it with the filling data.If there are less missing data i will remove the rows.': 1,\n",
       "  'binning': 0,\n",
       "  'missing data will be replaced with mean of height': 1,\n",
       "  'mean of the height': 1,\n",
       "  'Mean of height': 1,\n",
       "  'By attribute mean': 1,\n",
       "  'we could fill it with mean value or use interpolation method if data is sorted': 1,\n",
       "  'Replace with mean/median': 1,\n",
       "  'attribute mean of the same class': 1,\n",
       "  'global constant , mean or mode': 1},\n",
       " 'Name 2 curses of dimensionality': {'dont know': 0,\n",
       "  'Anomaly Detection, Dimensionality Reduction': 0,\n",
       "  'Data Sparsity and Distance measure': 1,\n",
       "  'data soarsity & distance concentration': 0,\n",
       "  'High dimensional data, Sparse data': 1,\n",
       "  '1.PDF-Probability Distribution function,CDF-Cumulative Distribution function 2. Inter and Intra class': 0,\n",
       "  'data sparsity and distance concentration': 1,\n",
       "  'Distance concentration and data sparsity': 1,\n",
       "  \"Can't answer\": 0,\n",
       "  'Training time and amount of data required for training, Local but globally suboptimal solution.': 1,\n",
       "  'High dimensionality can lead to confusion for the model (Dimensionality reduction), Distance comparison cannot be performed.': 0,\n",
       "  '1)More samples required.2) visualizations.3)space coverage.': 2,\n",
       "  'data sparsity and data concentration': 1,\n",
       "  '1. Count of Parameters in the feature vectors ... 2. Dimensionality of the Vector space.. 3. the number of attributes that the object possesses': 0,\n",
       "  'Computation will increase exponentially with increase in dimension.': 1,\n",
       "  '1. More dimensions, computational efficiency decreases 2. More dimensions means visualization become a problem after when dimensions is more than 3': 2,\n",
       "  'Increase in number of1. increased errors like overfitting and underfitting.2.It can also lead to wastage of space to store reduntant information(features)': 2,\n",
       "  'Data can be lost and classification is also a little bit difficult': 0,\n",
       "  'Visualizationg challenges and densitiy estimation ': 2,\n",
       "  'Manathan, Hamming': 0,\n",
       "  'data sparsity, distance concentration': 1,\n",
       "  'Data sparsity , Distance concentration': 1,\n",
       "  'Data sparsity and distance concentration': 1,\n",
       "  'increase in error and increase in number of features': 0,\n",
       "  'Larger the Dimension it is difficult to extract the features. difficult to analyze the data': 0,\n",
       "  'performance challenge and complexity': 2,\n",
       "  'Count of parameters , # of attributes ': 0,\n",
       "  '1 need more computational time. 2 higher dimensionality need higher space coverage': 2,\n",
       "  'Density estimation , No of samples needed will increase as S=P*s^D In order to get higher space coverage the dimension incresases': 1,\n",
       "  'Distance Concentration and Data Sparsity': 1,\n",
       "  'Having too many features/attributes, increases dimensionality - 1. leads to more sparsity/missing values. 2. concentration of data i.e occupies most space in higher dimension compared to lower dimensions for the same data, this leaves less space for the rest of data.': 1,\n",
       "  'dimentionality reduction, regularisation': 0,\n",
       "  '1.Overfitting of the model ,2. No meaningful clusters can be formed': 1,\n",
       "  'Data sparsity and distance.': 1,\n",
       "  '1.sparsity of data 2. increase in the distance calculation.': 1,\n",
       "  'data sparcity and distance concentration': 1,\n",
       "  'More space coverage,density estimation challenges,Time consuming,performance challenges': 2,\n",
       "  'As dimension increases it is tough to visualise, offers poor space coverage': 2,\n",
       "  'Dimensionality Reduction Regularisation,mechine learnig PCA': 0,\n",
       "  'if the error increases if the number of feature increases ': 0,\n",
       "  'more information can be stored but results in higher noise. More redundancy in data': 0,\n",
       "  'data sparsity': 1,\n",
       "  'Data sparsity': 1,\n",
       "  'Euclidean distance and density estimation.': 0,\n",
       "  '1. number of samples required to represent higher dimension will be more, 2. computational time is high, 3. more memory space': 2,\n",
       "  'principal component analysis, singular value decomposition': 0,\n",
       "  'Data Sparsity, Distance concentration': 1,\n",
       "  'data sparsity, distance concentration ': 1,\n",
       "  'Principle component Analysis and Single value decomposition': 0,\n",
       "  'Map location': 0,\n",
       "  'Data Sparsity and Distance concentration': 1,\n",
       "  'error increases as feautures increases,': 0,\n",
       "  '1)Higher dimensions mean many attributes or features which becomes difficult to handle for finding insights and patterns for classification so error increases with increase in number of features and overfitting may result': 1,\n",
       "  ' dimensionality curse refer to the problems organizing data in high dimesnion space that do not occur in low dimension1.data sparsity 2. distance concentration': 1},\n",
       " 'What is the mean value of data after Z-score normalization is applied?': {'0': 1,\n",
       "  '0 (Zero)': 1,\n",
       "  'zero': 1,\n",
       "  'Mean becomes 1 standard deviation becomes 0': 0,\n",
       "  '0/zero': 1,\n",
       "  \"Can't answer\": 0,\n",
       "  'Zero': 1,\n",
       "  'one': 0,\n",
       "  '1': 0,\n",
       "  \"V'*standard deviation/ v\": 0,\n",
       "  'if it is below mean then it is -ve and above means +ve ': 0,\n",
       "  'mean is  set to zero': 1,\n",
       "  'means it set to zero': 1,\n",
       "  '0(zero)': 1,\n",
       "  'zero-one ': 0,\n",
       "  'V-u1/(sigma)': 0,\n",
       "  'Median value.': 0,\n",
       "  'Zero(0)': 1,\n",
       "  'It becomes near to normal distribution.': 0,\n",
       "  'Mean is 0': 1,\n",
       "  'based on mean and standard deviation': 0,\n",
       "  'Mean value will be zero': 1,\n",
       "  'z=(x- mean)/standard deviation where z is the mean of z-score normalization': 0,\n",
       "  'Formula fo z - score normalization is v = (v - mean of a)/ standard deviation of a .': 0,\n",
       "  'After z score normalistion, we have zero mean and standard deviation as one': 1,\n",
       "  'it ranges between 0 to 1': 0,\n",
       "  '0 (zero)': 1,\n",
       "  'The mean value of data after Z score normalization  will be 1.': 0,\n",
       "  'mean value is always one ': 0,\n",
       "  'z score': 0},\n",
       " 'Provide 2 examples of unsupervised machine learning.': {'Baby can identify dogs based on past learning,DNA Paaterns': 1,\n",
       "  'KNN, neural network': 0,\n",
       "  'KNN,Neural Networks.': 0,\n",
       "  'K means clustering, deep learning': 1,\n",
       "  'Genetics, anamoly detection, clustering, ': 1,\n",
       "  'PCA, KNN ,K-means clustering.': 1,\n",
       "  'k means ': 1,\n",
       "  '1.separating based on certain age groups. 2. customer segmentation based on their recent purchases': 1,\n",
       "  'Bank loans system, Detecting credit card fraud ': 0,\n",
       "  'Clustering, Association': 1,\n",
       "  'Dimensionality reduction(PCA) and Genetic algortihm(DNA)': 0,\n",
       "  'k mean clustering, density based clustering(DB scan)': 1,\n",
       "  '1. K-means of clustering algorithm , 2. KNN': 1,\n",
       "  'K-Means Clustering, KNN': 1,\n",
       "  'Cats and Dogs': 0,\n",
       "  'k-mean clustering,Principle Component Analysis': 1,\n",
       "  'Knn and svm': 0,\n",
       "  'KNN , K-Mean clustering': 1,\n",
       "  '1. Understanding the DNA patterns in the Genetics  2.Fraudulent detection in the network': 1,\n",
       "  'k means clustering,principal component analysis': 1,\n",
       "  'knn and neural networks': 0,\n",
       "  'Knn and neural networks': 0,\n",
       "  'market basket analysis ,semantic clustering': 1,\n",
       "  'k means clustering and neural networks for classification': 1,\n",
       "  'Recommendation System and Customer Segmentation.': 1,\n",
       "  'Kmean clustering, clustering analysis': 1,\n",
       "  'KNN (k-nearest neighbors),Neural Networks, clustering DNA patterns to analyze evolutionary biology,Audience segmentation': 1,\n",
       "  '1. Customer behaviour identification - for clustering customers based on few attributes 2. Medical Image Grouping based on properties in image 3. online shopping purchase suggestions': 1,\n",
       "  'Customer segmentation for market baskey analysisClassifying students based on height and weight using KNN': 1,\n",
       "  'Customer Segmentation, Recommendation systems - using algos like Kmeans or Heirarchial': 1,\n",
       "  'Principal component analysis, k-means clustering': 1,\n",
       "  'K-means Clustering, Principal Component Analysis, Hierarchical Clustering': 1,\n",
       "  'urban sustaininbility': 0,\n",
       "  'K-nearest Neighbors , Neural Networks': 0,\n",
       "  'Recommendation System, Customer segmentation': 1,\n",
       "  'clustering, apriori': 1,\n",
       "  'K-means and Clustering': 1,\n",
       "  'Nueral networks, Knn': 0,\n",
       "  'KNN ; Apriori  ': 0,\n",
       "  '1knn 2 clustering': 1,\n",
       "  '1. Identifying the fault based on Vehicle signal flowing 2. Identifying the animals detection  within pedestrians detection feature ': 0,\n",
       "  'K NN Classification, Association Rule Learning': 1,\n",
       "  'K-means clustering. KNN (k-nearest neighbors)': 1,\n",
       "  \"unsupervised machine learning are those which doesn't have a output label in the dataset. one example can be anamoly or outlier detection using knn algorithm. Image classification or of medical images, or MNSIT using Neural Networks can be example of unsupervised machine learning\": 0,\n",
       "  'image recognition': 0,\n",
       "  'Customer segmentation and weather condition': 1,\n",
       "  '1.customer segmentation 2.DNA patterns to analyze evolution': 1,\n",
       "  '1.Understanding different customer groups etc..<for marketing/Business> 2.In the field of Medical for clustering DNA samples or species': 1,\n",
       "  'Image classification, clustering ': 1,\n",
       "  'anomaly detection, dna patterns to analyze evolutionary biology': 1,\n",
       "  'algorithms- KNN,neural networks,clustering etc.Practical examples:Industrial production systems, e-commerce websites to find user-specific recommendation products': 1,\n",
       "  'there is no label in unsupervised machine learning . example :-clustering and principle component analysis': 1,\n",
       "  'K-means clustering, KNN (k-nearest neighbors), Hierarchal clustering.': 1,\n",
       "  'KNN,NEURAL NETWORKS, ANOMALY DETECTION': 0,\n",
       "  'Clustering, Anomaly detection': 1,\n",
       "  'k-means clustering , hierarchical clustering': 1},\n",
       " 'Support vector machine is a parametric classifier. Explain how this is true.': {'SVM are linear classifiers': 1,\n",
       "  'not true, as SVM is not fixed to certain fixed number of parameters ': 0,\n",
       "  \"It might be both, based on how I define parametric and non-parametric models. Parametric models have a fixed number of parameters that are independent of the amount of the dataset. Non-parametric models are anything that isn't a parametric model. It also has to do with the learning algorithm you employ.\": 0,\n",
       "  'In svm, we specify kernel function which makes assumption about the spread fo the data, so if data is assumed to be linearly separable, then we use basic linear kernel, if it is assumed to be radially spread then we use RBF kernel, hence we specify the parameter which assumes the spread of the data': 1,\n",
       "  'Because basic support vectors machines are linear classifiers.': 1,\n",
       "  'Beacause SVM is a linear classifier': 1,\n",
       "  'yes because it is a linear classifier its not consider a number which are non parametric': 1,\n",
       "  'support vectors are linear classifiers.': 1,\n",
       "  'number of dimensions and Margin are the parameter which SVM rely on. ': 1,\n",
       "  'In simple problem SVM classifies data linearly and fixed size of parameters, so parametric classifier.': 1,\n",
       "  'The main reason is because SVM can be use as a linear classifier and can be limited by the size of parameter': 1,\n",
       "  'svm is a parametric model because  parametric models are something with fixed finite number of parameters independent of data set size': 1,\n",
       "  'This is because basic support vector machines are linear classifiers. ': 1,\n",
       "  'SVM can be done on nonlinear data, and if we have SVM working on data given by a finite number of parameters, then it is nonparametric.': 0,\n",
       "  'It is fixed with number of parameters independent of dataset size.': 1,\n",
       "  'The basic SVM are liner classifiers': 1,\n",
       "  'Because they are linear classifiers': 1,\n",
       "  'it is non parametric , parametric model give probability distribution data for finite numbers. SVM model dont deal with underlying probabilistic model they also dont have the matric to compute distance': 0,\n",
       "  'SVMs is a parametric classifier because support vector classifiers are linear classifiers and are constrained by the number of parameter.s': 1,\n",
       "  'works with fixed number of parameters independent of dataset size': 1,\n",
       "  'because svm are linear classifiers they are parametric classifiers': 1,\n",
       "  'It is parametric because it is a linear classification technique  and it is constrained by a set of parameters like gamma and regulization parameter that is c': 1,\n",
       "  'based on hyperplane and margin': 0,\n",
       "  'this is true because in SVM we have finite number of parameters that are used': 1,\n",
       "  'Because in SVM we already know the classes in which the output can fall.': 0,\n",
       "  'It takes output from linear classfiers': 0,\n",
       "  'Basic support vector machines are linear classifiers. SVMs that are not constrained by a set number of parameters are considered non-parametric.': 0,\n",
       "  'parametric classifier uses certain assumptions from the available data distribution pattern. training samples are not required': 1,\n",
       "  'Because it takes parameters such as regularisation parameter and  weight, bias and input in the form w.x + b': 1,\n",
       "  'It is not parametric as it does not have a specific function for classification of classes.': 0,\n",
       "  'While developing the SVM model the maximization of margin is required, and in this optimization problem there are some constraints which makes it a parametric classifier.': 1,\n",
       "  'This is because basic support vector machines are linear classifiers.': 1,\n",
       "  ' d': 0,\n",
       "  'The support vector machines are the linear classifeirs and hence the linear classifires pass on paramenters we can conclude that for basic svm it is parametric classifer .': 1,\n",
       "  'SVM uses two parameters -  regularization and Gamma parameters. Used in kernel trick. for Non-linear separable classes': 1,\n",
       "  'finite number of parameters are present and it is a linear classifier': 1,\n",
       "  'Because basic support vector machines are linear classifiers.': 1,\n",
       "  'Basic SVM classifier is a Linear classifier, so it is parametric.': 1,\n",
       "  'Support vector machines are linear classifier  ': 1,\n",
       "  'SVM USES  ': 0,\n",
       "  'SVM linear classifier is built from training data, for which classifications are known ': 0,\n",
       "  'Support Vector uses Regularization Parameter C and Hyperplane Positioning Gamma which will decide the Class boundary and hence will affect the classification.Hence': 1,\n",
       "  'his is because basic support vector machines are linear classifiers': 1,\n",
       "  'svm classifes the classes in a linear way or it can be said that svm is a linear classifier so it is a parametric classifier. ': 1,\n",
       "  '-------------------------------------': 0,\n",
       "  'SVM is a parametric classifier when it is linear as it has 1D parameters(w,b)': 1,\n",
       "  'Basic SVM model is linear classifier, such as parametric classifier. Advanced SVM can work with non-linear data which will be nonparametric.': 0,\n",
       "  \"As the value of y'=depends on two parmeters (sign, and score)\": 0,\n",
       "  'Because SVM is linear classifier': 1,\n",
       "  'svm involves penalty parameter and kernel parameter , svm classifier is analyzed when these parameters take different values ': 1,\n",
       "  'Tt is parametric as C and gamma parameters influence classification.': 1,\n",
       "  'support vector machine capture all information about the data within its parameter and predict the new data': 1,\n",
       "  'Basic SVM are linear classifiers, and as such parametric algorithms. Advanced SVM can work for nonlinear data, and if you have a SVM working for data not constrained to be in a family described by a finite number of parameters, then it is nonparametric': 1,\n",
       "  'fixed finite number of parameters independent of data set size': 1,\n",
       "  'It is a parametric classifier because they are linear classifiers.': 1,\n",
       "  \"True, because SVM's are linear classifiers.\": 1},\n",
       " 'What is the impact of not normalizing the attributes, in a set of feature vectors, on distance calculation? Provide your answer in a single or two sentences.': {'if we dont normalize the attributes in a set of feature vectors , on calculation distance then we get biased results as the one which has higher values compared to others will try to dominate our calculation results.': 2,\n",
       "  '-': 0,\n",
       "  'Not normalizing a data would lead to wrong prediction of answers as normalizing can get the correct picture of data in which we can easily analize.l': 0,\n",
       "  'The large attribute may affect the other attributes due to that error may occur or imbalance is created': 1,\n",
       "  'wrong prediction caused by the difference in a single attribute even though all the remain are same. More weighted attribute which was not normalized will cause the false value ': 2,\n",
       "  'normalizing helps in computations of attributes and we can bring down larger values between 0 to 1 which makes our analysis easier. ': 1,\n",
       "  'large number of data which leads to noisy data and inconsustency': 0,\n",
       "  'the number of attributes will be more.': 0,\n",
       "  'IMPACT: if we do not normalize we might have to deal with values that give a very large mean just because of the presence of one or two features with a very large value. Hence normalization helps in getting a more accurate central tendency': 1,\n",
       "  'Normalization helps to solve easier and makes the data so that distance calculation becomes easier. ': 0,\n",
       "  'Impact of not normalizing the attributes in a set of vectors may lead to data redundancy and ur system may not be efficient': 0,\n",
       "  'the attributes will be scattered': 0,\n",
       "  'Plotting graphs and analysis may be very hard also normalizing attributes can have more zeroes as elements that leads to faster calculations': 1,\n",
       "  'Normailzation is used to handle huge values of data by scaling them so they can be used in forms of vectors and distance calculation.': 0,\n",
       "  'it is qualitative and quantitative ': 0,\n",
       "  'if we dont normalize we cant do the distance calculations': 0,\n",
       "  'If we had not normalized the attributes in feature vectors based on the distance calculation then we cant able to predict the new attribute category like which category the new attribute will belong to.': 0,\n",
       "  'it will be hard for to map the problem efficiently,': 0,\n",
       "  \"Due to high difference between range of values of features, we may get error in similarities and hence can't classify accurately. Eg: Range of Height can be (100 to 200) whereas Income can be (10000 to 200000)\": 2,\n",
       "  'we can say that it is a quantative and qualitative': 0,\n",
       "  'The impact of not normalizing features vectors for distance calculation will be some of the attributes will be contributing more and some will be contributing less but it might be possible less contributing features has more importance or has not been to give his proper representation. this will lead to the higher percentage of errors.': 2,\n",
       "  \"if we dont normalize we couldn't get the nearest neighbour values so we can't lead to the desicion\": 0,\n",
       "  'larger attribute will affect the other attributes': 0,\n",
       "  'the large attribute may affect the other due to that  error may occur  ': 0,\n",
       "  'if normalization is not done the mean might be very high': 0,\n",
       "  'The number of attributes will be more so it takes more computational time to get the solution.': 0,\n",
       "  'the calculation part  required will be more': 0,\n",
       "  'The impact of not normalizing the values as we can not arrive at a clear decision if the data is not normalized and we can not find the median properly ': 0,\n",
       "  'Attributes should be normalised using using min max or z score normalisation because then only data will be standardized and distance can be calculated otherwise it is difficult to calculate distance for random numbers': 1,\n",
       "  'Normalizing makes graphing difficult': 0,\n",
       "  'When we do not normalize. there occurs problems like we cannot calculate the new attributes, new features efficiently.': 0,\n",
       "  'the normali': 0,\n",
       "  'if normalization was not done using any of methods, min max or z score method, data will standardize and distance may not be calculated': 1,\n",
       "  'to enhance the accuracy of the of the data and to reduce the errors,hence we need to normalize the attributes  ': 0,\n",
       "  'If we use KNN for data which has high values(weighted) it will be difficult to classify so we will have to normalize to bring all the values to a lower value hence allowing us to make a better prediction.': 0,\n",
       "  'the impact of not normalizing feature vectors for distance calculation will be some of the attributes will be contributing more and some will be contributing less but itmight be possible toless contributing feature has more importance orhas not been togive his proper representation . this willlead to the higher percentage of error ': 2,\n",
       "  'NORMALIZATION GENERALLY USED TO HANDLE HUGE VALUES OF VECTORS  BY THEM WE CAN SOLVE THEM': 0,\n",
       "  'normalization is used to handle the huge values': 0,\n",
       "  'Normalisation is done to reduce data redundancy and improve data integrity. It also helps in the ensuring of checking similar records.': 0,\n",
       "  'the part of  calculation required will be more ': 0,\n",
       "  'The range of values over which we are operating without normalization can vary very vastly due to presence outlies. Without normalization, outliers can heavily impact the output during distance calculation.': 2,\n",
       "  'The value of the distance might vary based on the value of the attributes. So we must normalize the attributes': 0,\n",
       "  ' To ensure that data is similar across all records.': 0,\n",
       "  ' When we normalize data, we construct tables based on specific rules.data normalization is to ensure that data is similar across all records': 0,\n",
       "  'The calculation required will be more.': 0,\n",
       "  'NORMALISATIONGENERALLY USED TO HANDLE LARGE DATA': 0,\n",
       "  'not normalizing the attributes': 0,\n",
       "  'larger attributes effect the other attribute': 0,\n",
       "  'In this case the calculation to be done will be more ': 0,\n",
       "  'The range is in between 0 and 1': 0,\n",
       "  'one attribute may become dominent and give incorrect distances': 2,\n",
       "  \"if we don't normalize the data there will be lots of errors and noice in the data created and leads to wrong answer\": 0,\n",
       "  'larger attributes effect the other attributes': 0,\n",
       "  \"We find more errors and noise in the given data if we don't normalize the attributes which further leads to wrong outputs.\": 0,\n",
       "  'with out normalising we cant find the distance we got of the data/feature vectors are correct or not before and after normaising given data': 0,\n",
       "  'the complexity of calculation increases ,normalization decreases the variance which helps you out': 1,\n",
       "  'if we do not normalize the attributes the data which we get are not equal results. so we cannot calculate the distance . ': 0,\n",
       "  'one attribute may become dominant and give incorrect distances': 2,\n",
       "  'one attribute may become dominant and give incorrect distances . hence normalization is required to get rid of this error.': 2,\n",
       "  'accuracy may increase': 0,\n",
       "  'aewrg': 0,\n",
       "  'Normalization avoids distortion of data. It also improves the performance. Accuracy is increased. Using various techniques and algorithms we can improve the quality of data.': 0,\n",
       "  'complexity of the calculation increases': 0,\n",
       "  'not normalizing the attributes may not be able to equal/similar results as normalizing does': 0},\n",
       " 'Name an approach to avoid overfitting in decision tree': {'Prepruning ': 1,\n",
       "  'prepruning,postpruning': 1,\n",
       "  'Tree pruning - ( prepruning or postpruning )': 1,\n",
       "  'prepruning': 1,\n",
       "  'Pruning': 1,\n",
       "  'postpruning': 1,\n",
       "  'Post pruning and pre-pruning': 1,\n",
       "  'Pre-pruning': 1,\n",
       "  'pruning': 1,\n",
       "  'prepurning': 1,\n",
       "  'prepruning, postpruning': 1,\n",
       "  'postpruing ': 1,\n",
       "  'Preprunning , Postprunning': 1,\n",
       "  'Prunning (Can be both pre prunning and post prunning)': 1,\n",
       "  'prepruning or postpruning': 1,\n",
       "  'Postpruning': 1,\n",
       "  'Preprunning.': 1,\n",
       "  'Prepruning': 1,\n",
       "  'prepruning and postpruning': 1,\n",
       "  'Pruning  (Pre and post pruning)': 1,\n",
       "  'to avoid overfitting, we can use prepruning  or postpuning.': 1,\n",
       "  'pre-pruning': 1,\n",
       "  'Post pruning': 1,\n",
       "  'preprunning': 1,\n",
       "  'preprunning,postpruning': 1,\n",
       "  'post pruning': 1,\n",
       "  'Prepruning, postpruning': 1,\n",
       "  'Cross-validation': 0,\n",
       "  'Prunning ( Pre and Post )': 1,\n",
       "  'Pruning (pre-pruning or post-pruning)': 1,\n",
       "  'prepruning / postpruning': 1,\n",
       "  'Prepruning, Postpruning': 1,\n",
       "  'pre pruning and post pruning': 1,\n",
       "  'Prepruning and postpruning can be used.': 1,\n",
       "  'Prepruning,Postpruning': 1,\n",
       "  'cross validation': 0,\n",
       "  'pre/ post prunning': 1,\n",
       "  'Prepruning and  post pruning': 1,\n",
       "  'prepruning and ost pruning': 1,\n",
       "  'PrePruning and PostPruning': 1,\n",
       "  'Prepruning and Postpruning': 1,\n",
       "  'post-pruning': 1,\n",
       "  'Pre pruning': 1,\n",
       "  'Prepruning and postpruning': 1,\n",
       "  'Preprunning': 1,\n",
       "  'Prepruning or postpruning method': 1,\n",
       "  'pruning /pre-pruning /post-pruning': 1,\n",
       "  'Postpruning or Prepruning': 1,\n",
       "  'tree prunning': 1,\n",
       "  'Prepruning and post-pruning': 1,\n",
       "  ' postPruning': 1,\n",
       "  'Prepruning and postpruning are two approaches to avoid overfitting': 1,\n",
       "  'pre pruning: stop the tree growth prematurely': 1,\n",
       "  'pre pruning': 1,\n",
       "  'traning with more data': 0,\n",
       "  'pruning,  post pruning, pre pruning': 1,\n",
       "  'Pruning is an approach that is used to avoid over fitting in decesion tree': 1,\n",
       "  'pre- pruning ,  cross validation , data augmentation': 1,\n",
       "  'Prunning.': 1,\n",
       "  'we can define pre Pruning as one approach.': 1,\n",
       "  'cross-validation': 0,\n",
       "  'Prepurning and post purning': 1,\n",
       "  'preprunin': 1},\n",
       " 'Name any one attribute selection measure other than information gain': {'Gini index': 1,\n",
       "  'gini index': 1,\n",
       "  'Gain ratio': 1,\n",
       "  'Mutual information': 1,\n",
       "  'entropy': 0,\n",
       "  'Gini Index': 1,\n",
       "  'gain ratio': 1,\n",
       "  'Gini ratio, entropy': 1,\n",
       "  'entropy , gini ': 1,\n",
       "  'gini index,gain ratio': 1,\n",
       "  'ratio gain': 1,\n",
       "  'mutual information': 1,\n",
       "  'Gini Index (default)': 1,\n",
       "  'Entropy': 0,\n",
       "  'Gain Ratio ': 1,\n",
       "  'Mutual Information ': 1,\n",
       "  'Gain ratio, Gain index.': 1,\n",
       "  'gain ratio ': 1,\n",
       "  'gini index, entropy': 1,\n",
       "  'gain ratio, gini index': 1,\n",
       "  'Gain ratio, gini index': 1,\n",
       "  'Gain Ratio': 1,\n",
       "  'Gain ratio,gini index': 1,\n",
       "  'gain ration': 1,\n",
       "  'gain ratio or gini indexing': 1,\n",
       "  'Gini index, gain ratio.': 1,\n",
       "  'gini': 1,\n",
       "  'wrapper method': 0,\n",
       "  'gini impurity': 1,\n",
       "  'Gain ratio(S,A)': 1,\n",
       "  'gain ratio or ginni  index': 1,\n",
       "  'Gini Index, Gain Ratio': 1,\n",
       "  'Mutual Information': 1,\n",
       "  'gain': 1,\n",
       "  'Gini index and Gain Ratio': 1,\n",
       "  'gini index ': 1,\n",
       "  'gain ratio , gini index': 1,\n",
       "  'gini index.': 1,\n",
       "  ' Gini index': 1,\n",
       "  'Gini Index and Gain ratio': 1,\n",
       "  'Information gain': 0},\n",
       " 'What is the inductive bias in decision trees?': {'Shorter trees are much preferred than longer trees i.e., the trees that have high info gain near root': 1,\n",
       "  'shorter trees are prefered over longer trees': 1,\n",
       "  'The inductive bias in decision trees is that the shorter trees are preferred over larger trees.': 1,\n",
       "  'indepepreference to shorter trees are given more than larger trees': 1,\n",
       "  'Shorter trees are preferred over larger trees.': 1,\n",
       "  'Shorter trees are preferred over longer trees': 1,\n",
       "  'Shorter trees are preferred over large trees': 1,\n",
       "  'Inductive bias is an assumption in decision trees which keeps features with higher information gain towards the tree root, in order to make the decsion tree smaller and more effecient.': 1,\n",
       "  'to make longer trees into smaller ones to improve accuracy of the model': 1,\n",
       "  \"It is a consequence of the ordering of a tree w.r.t the search strat. Trees that place high information gain attributes close to the root are preferred over those which don't\": 0,\n",
       "  'it is for using shorter trees instead of larger trees': 1,\n",
       "  'shorter trees are preferred over larger trees': 1,\n",
       "  'trees which have high information gain are taken over the others': 0,\n",
       "  'Shorter trees are preferred over long trees': 1,\n",
       "  'inductive bias is a set of assumptions to perform induction': 0,\n",
       "  'The trees that have high information gain attribute close to the root are preferred.': 0,\n",
       "  'The bias is that longer trees are less prefered , that is we use shorter trees when compared to longer trees': 1,\n",
       "  'shorter trees are considered over longer trees': 1,\n",
       "  'which among the  tree is shorter it is preferred over larger trees': 1,\n",
       "  'set of assumptions we use to predict outputs': 0,\n",
       "  'shorter trees are preffered over larger trees': 1,\n",
       "  'shorter decision trees are preferred over larger ones, as they tend to generalize rather the overfit': 1,\n",
       "  'assumtions that we make to outputs with the given set of inputs': 0,\n",
       "  'The attribute having high information gain closer to root node ': 0,\n",
       "  'Inductive bias is nothing but a learning bias which the learner uses to predict an accurate output based on the given inputs. Coming to trees , shorter trees are chosen over the trees which are huge .': 1,\n",
       "  'we prefer shorter tress over the longer trees as in descision': 1,\n",
       "  'the assumptions we take to predict the output': 0,\n",
       "  'short trees are preferred over long trees. Roots are preferred to have information gain attributes': 1,\n",
       "  'Inductive bias says that shorter or trees with less depth are preferred over larger trees.': 1,\n",
       "  ' Inductive bias in decision tree means  we  prefer for shorter trees than  larger trees': 1,\n",
       "  'Trees with information closer to the root are prefered over trees with information further away from the node': 0,\n",
       "  'inductive bias is something where the uncovered attribute inputs are used for getting the output. ': 0,\n",
       "  'the assumption we take, to get the output': 0,\n",
       "  'the trees that have high information gain attributes to the root are considered than other': 0,\n",
       "  'shorter trees are preffered over longer trees': 1,\n",
       "  'attributes which are closer have high information gain': 0,\n",
       "  'shorter tree are preferred over longer tree': 1,\n",
       "  'set of instructions made by learnng algorithm in order to perfom induction': 0,\n",
       "  'shorter trees are prefred over longer trees': 1,\n",
       "  'inductive bias is a set of assumptions that ': 0,\n",
       "  'Trees that give more information gain are placed closer to the root are preferred over those that are not. In short, Shorter trees are preferred over longer trees.': 1,\n",
       "  'Shorter trees are preferred over longer trees.': 1,\n",
       "  'inductive bias in decision tree iswe can choose shorter trees(heightof tree) are used over longer trees': 1,\n",
       "  'Shorter trees are preferred over larger trees': 1,\n",
       "  'Short trees are preferred over long trees': 1,\n",
       "  'In this inductive bias shorter trees are preferred over longer trees.trees that place high information gain attributes close to the root aare preferred over those  that do not ': 1,\n",
       "  'Certain assumptions made to predict output like shorter tress are preffered over larger trees.': 1,\n",
       "  'It means that trees with lesser depth (short) are more preferred as they have hight information gain near the root': 1,\n",
       "  'Trees which best fit the data and are shorter are preferred over large trees.': 1,\n",
       "  'shorter trees prefered over long trees': 1,\n",
       "  'Inductive bias is the case where the model givespriority to the shortest decision tree that best fits the data among all the possible decision trees': 1,\n",
       "  'Basically Shorter trees are preffered over longer trees': 1,\n",
       "  'shortest trees are preferred over larger trees': 1,\n",
       "  'set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered.': 0,\n",
       "  'multiple attributes': 0,\n",
       "  'shorter tress are more preferrable than longer tress': 1,\n",
       "  'the tress with least depth is better than longer tress (shorter tress are preferred over longer tree )': 1,\n",
       "  'short trees have higher preference over long trees': 1,\n",
       "  'Short trees will be preferrable over long trees. It is used to predict outputs from the  inputs that it has not encountered.': 1,\n",
       "  'may we have to select the tree which are shorter because it means like well classified and generalized like we gain max information': 0,\n",
       "  'assumptions made by a learning algorithm in order to perform induction,where shorter trees are preferred': 1,\n",
       "  'shorter trees are more prefferd than larger trees': 1,\n",
       "  'The indictive bias is that the tree with short length is prefered over the tree with a longer length': 1,\n",
       "  'inductive bias is also called learning bias and it says  shorter trees are preferred over larger trees to avoid bias': 1,\n",
       "  'shorter trees  are  prefered over  larger trees': 1,\n",
       "  'shorter trees are preferred over larger trees ': 1,\n",
       "  'Assumptions that a learner uses to predict output for unseen inputs': 0,\n",
       "  \"Trees that have high information gain features closer to the root are given preference over those who don't.\": 0,\n",
       "  'inductive bais are assumptions or reasons that are taken into consideration when predicting a given output from inputs': 0,\n",
       "  'inductive bias is the set of assumptions or constraints we impose that the we use to predict outputs of given inputs . like we prefer shorter trees instead of longer trees for our decision making ': 1,\n",
       "  'it states that shorter trees are more preferable than larger trees': 1,\n",
       "  \"Choosing shorter trees over larger trees, as it is more efficient if it doesn't make much difference in accuracy.\": 1,\n",
       "  'to prefer the short tress over large trees': 1,\n",
       "  'shorter trees are prefered over larger trees': 1,\n",
       "  'it is a set assumptions that are useful predict the output': 0,\n",
       "  'shorter trees are refrerred over larger trees': 1,\n",
       "  'Short trees are more preferred than Longer trees.': 1,\n",
       "  'it will help to predict output': 0,\n",
       "  'short trees are better': 1,\n",
       "  'shorter trees are preferred over longer/larger trees ': 1,\n",
       "  'it means that shorter trees are preferable over larger trees.': 1,\n",
       "  'Shorter trees are preferred over longer ones': 1,\n",
       "  'assumption the learner used to predict': 0,\n",
       "  'Inductive bias  is the set of assumptions that a learner uses to predict outputs from given inputs that it has not encountered. In decision trees, it would prefer shorter tress over larger trees.': 1,\n",
       "  'Inductive bias is set of assumptions that learner use to predicts output of given input that has not encountered': 0,\n",
       "  'Inductive bias is an assumption made by us for the output prediction for the given input': 0,\n",
       "  'shorter trees are chosen over larger trees': 1,\n",
       "  'the decision trees that have high information gain attributes nearer to its root are given more priority.': 0,\n",
       "  'Inductive bias is the assumptions we make while constructing the decision tree, basically shorter trees are preferred over larger ones': 1,\n",
       "  'it means shorter trees are chosen over longer trees in decision trees.': 1,\n",
       "  'Inductive bais are the assumptions that are made by user to make predictions on the data which is not yet encountered.': 0,\n",
       "  'means it prefers short trees over larger trees': 1,\n",
       "  'Height of the Decision tree selection': 0,\n",
       "  'shorter trees are prefered over longer ones': 1,\n",
       "  'dont know': 0,\n",
       "  'They are assumptions made by the learner to predict outputs for input cases not yet encountered during training': 0,\n",
       "  'trees that has high information gain, that attributes close to the root are considered than other, shorter trees considered': 0,\n",
       "  'here, shorter trees are preferred over larger trees': 1,\n",
       "  'Shorter trees are taken over longer ones': 1,\n",
       "  'shorter trees are preferred over longer trees': 1,\n",
       "  'aussumption made by learning algorthim': 0,\n",
       "  'set of assumtions we use to predict outputs': 0,\n",
       "  'Trees which are shorten given high preference rather than long trees': 1,\n",
       "  'It is set of assumptions where learnenr use to predict the output of the input which was not encountered': 0,\n",
       "  'the inductive basis is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered': 0,\n",
       "  'shorter trees are prefered over longer trees.': 1,\n",
       "  'shorter tress are prefered over larger trees': 1,\n",
       "  'In trees the attributes which have high information gain are placed close to the root and are more prefered then that of not much gain.': 0,\n",
       "  'shorting': 0,\n",
       "  'shorter trees are preferred over the longer trees is the inductive bias': 1,\n",
       "  'Shorter trees have more importance when compared to longer ones': 1,\n",
       "  'Short trees are preferred over large trees': 1,\n",
       "  'Shorter trees would be preferred over longer ones': 1,\n",
       "  'shorter trees are given higher priority than longer trees': 1,\n",
       "  'shorter tree prefer over longer trees': 1,\n",
       "  'shorter tress': 1,\n",
       "  'Shorter trees are preferred over larger trees-10': 1}}"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6543d90a",
   "metadata": {},
   "source": [
    "### Train the model with answers as input and grades as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "774a3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "grades = []\n",
    "for i in QA.values():\n",
    "    temp1 = list(i.keys())\n",
    "    temp2 = list(i.values())\n",
    "    for (j,k) in zip(temp1,temp2):\n",
    "        answers.append(remove_stop_words(j))\n",
    "        grades.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "ad388bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[31, 25],\n",
       " [20, 36],\n",
       " [24, 35],\n",
       " [29, 30],\n",
       " [25, 3, 2],\n",
       " [3, 2, 25],\n",
       " [16, 17],\n",
       " [20, 13],\n",
       " [15, 19],\n",
       " [17, 17],\n",
       " [3, 70],\n",
       " [48, 25],\n",
       " [43, 4, 23],\n",
       " [35, 35],\n",
       " [43, 27],\n",
       " [21, 105],\n",
       " [23, 103],\n",
       " [21, 26, 9],\n",
       " [21, 35],\n",
       " [16, 40],\n",
       " [15, 41],\n",
       " [54, 7, 5],\n",
       " [58, 6, 4],\n",
       " [48, 7, 9],\n",
       " [5, 134],\n",
       " [16, 123],\n",
       " [38, 101]]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "5235b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer1 = TfidfVectorizer()\n",
    "answer_vectors1 = vectorizer1.fit_transform(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "ad7203f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1280x1531 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8237 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_vectors1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "d0d7305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MLPClassifier(hidden_layer_sizes = (64,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "46867010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(64, 32))"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(answer_vectors1, grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "97c4f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = answer_vectors1\n",
    "y_train = grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "4b5dde8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.53125"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
    "train_accuracy = accuracy_score(y_train, model1.predict(x_train))*100\n",
    "train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "141a8a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<103x1531 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 806 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Directory = r\"D:\\Sem-6\\21AIE312 - DL\\Project Data\\DL-Dataset\\Testing\"\n",
    "data_dir = os.listdir(Directory)\n",
    "\n",
    "data_dir\n",
    "\n",
    "import numpy as np\n",
    "np.ceil(1.6)\n",
    "\n",
    "def read_processed_csv(file_directory):\n",
    "    df = pd.read_csv(file_directory)\n",
    "    cols = df.columns\n",
    "    for i in range(0,len(cols),2):\n",
    "#         print(cols[i+1])\n",
    "        df[cols[i+1]] = df[cols[i+1]].astype(\"float\")\n",
    "        df[cols[i+1]]=[int(np.ceil(j)) for j in list(df[cols[i+1]])]\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def Data_Count(points_list):\n",
    "    points = set(points_list)\n",
    "    counts = [points_list.count(i) for i in points]\n",
    "    return counts\n",
    "\n",
    "# Data_Count()\n",
    "\n",
    "Questions = []\n",
    "QA = dict()\n",
    "data_count = []\n",
    "for i in data_dir:\n",
    "#   print(i)\n",
    "    data_path = os.path.join(Directory, i)\n",
    "    csv_read = read_processed_csv(data_path)\n",
    "    cols = csv_read.columns\n",
    "#   print(cols)\n",
    "    for j in range(0,len(cols),2):\n",
    "        Questions.append(cols[j])\n",
    "        Answer_grades = dict()\n",
    "        answers = list(csv_read[cols[j]])\n",
    "        points = list(csv_read[cols[j+1]])\n",
    "        data_count.append(Data_Count(points))\n",
    "        for (k,l) in zip(answers, points):\n",
    "            Answer_grades[k] = l\n",
    "        QA[cols[j]] = Answer_grades\n",
    "\n",
    "zero_count = 0\n",
    "ones_count = 0\n",
    "twos_count = 0\n",
    "for i in data_count:\n",
    "    zero_count = zero_count + i[0]\n",
    "    ones_count = ones_count + i[1]\n",
    "    if len(i)==3:\n",
    "        twos_count = twos_count + i[2]\n",
    "\n",
    "zero_count\n",
    "\n",
    "ones_count\n",
    "\n",
    "twos_count\n",
    "\n",
    "num_questions = len(Questions)\n",
    "num_questions\n",
    "\n",
    "## Data count is a list of count of different grades with their indices indicating the grade\n",
    "len(data_count)\n",
    "\n",
    "QA\n",
    "\n",
    "### Train the model with answers as input and grades as output\n",
    "\n",
    "answers = []\n",
    "grades = []\n",
    "for i in QA.values():\n",
    "    temp1 = list(i.keys())\n",
    "    temp2 = list(i.values())\n",
    "    for (j,k) in zip(temp1,temp2):\n",
    "        answers.append(remove_stop_words(j))\n",
    "        grades.append(k)\n",
    "\n",
    "data_count\n",
    "\n",
    "# vectorizer1 = TfidfVectorizer()\n",
    "answer_vectors1 = vectorizer1.transform(answers)\n",
    "\n",
    "answer_vectors1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "a98ff61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = answer_vectors1\n",
    "y_test = grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "727f6953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "cef5b1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.46601941747572"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = accuracy_score(y_test,model1.predict(x_test))*100\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "0a4db626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 99.53%\n",
      "Testing Accuracy : 84.47%\n",
      "Precision : 0.84%\n",
      "Recall : 0.84%\n",
      "f1score : 0.84%\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, model1.predict(x_test), average='weighted')\n",
    "recall = recall_score(y_test, model1.predict(x_test), average='weighted')\n",
    "f1score = f1_score(y_test, model1.predict(x_test),average='weighted')\n",
    "print(\"Training Accuracy : {:.2f}%\".format(train_accuracy))ww\n",
    "print(\"Testing Accuracy : {:.2f}%\".format(test_accuracy))\n",
    "print(\"Precision : {:.2f}%\".format(precision))\n",
    "print(\"Recall : {:.2f}%\".format(recall))\n",
    "print(\"f1score : {:.2f}%\".format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "a5ae009d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEWCAYAAABLzQ1kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAadklEQVR4nO3deZxcVbnu8d/TGQiZyBwio5wTQFAJ3ohMYgiCgAM4IAh6czEaFRkURPEMeIB7zsWLIOIBJQgSQALhAIcwJcRIZCYTY0DAyxgSMnQgQAKS7n7vH7U7VppOVXXSVXtV9/Plsz9VtYe1327j26vevfbaigjMzCw9DXkHYGZm7XOCNjNLlBO0mVminKDNzBLlBG1mlignaDOzRDlB22aTtKWkWyWtlnTDZrRznKS7OjO2PEi6U9KEvOOw+ucE3Y1IOlbSfElvS1qaJZL9O6HprwAjgaERcdSmNhIRf4iIQzohng1IGicpJN3UZv0e2fo5Fbbzb5KuKbdfRBwWEVM2MVyz9ZyguwlJpwIXAv9BIZluD1wCHNEJze8APBsRTZ3QVrWsAPaVNLRo3QTg2c46gQr8/ynrNP7H1A1I2go4G/h+RNwUEWsiYl1E3BoRp2f7bCHpQklLsuVCSVtk28ZJWizpNEnLs9738dm2s4AzgaOznvnEtj1NSTtmPdWe2ef/Jel5SW9JekHScUXr7ys6bl9J87LSyTxJ+xZtmyPpHEn3Z+3cJWlYiV/De8B/A8dkx/cAvgr8oc3v6leSXpH0pqQFkj6ZrT8U+Kein/Oxojj+XdL9wFpgp2zdt7Ltv5H0X0Xt/1zSbEmq+H9A67acoLuHfYA+wM0l9vlnYG9gDLAHsBfwL0Xbtwa2ArYBJgIXSxocET+j0Cu/PiL6R8TlpQKR1A+4CDgsIgYA+wKPtrPfEOD2bN+hwAXA7W16wMcCxwMjgN7Aj0qdG7gK+J/Z+88Ai4AlbfaZR+F3MAS4FrhBUp+ImNHm59yj6JhvAJOAAcBLbdo7Dfho9sfnkxR+dxPCcyxYBZygu4ehwMoyJYjjgLMjYnlErADOopB4Wq3Ltq+LiDuAt4FdNjGeFuDDkraMiKURsaidfT4LPBcRV0dEU0RMBf4CfL5on99HxLMR8Q4wjUJi3aiIeAAYImkXCon6qnb2uSYiGrNzng9sQfmf88qIWJQds65Ne2uBr1P4A3MNcFJELC7TnhngBN1dNALDWksMG/EBNuz9vZStW99GmwS/Fujf0UAiYg1wNPBdYKmk2yXtWkE8rTFtU/T5tU2I52rgROBA2vlGkZVxns7KKm9Q+NZQqnQC8EqpjRExF3geEIU/JGYVcYLuHh4E3gWOLLHPEgoX+1ptz/u//ldqDdC36PPWxRsjYmZEHAyMotArvqyCeFpjenUTY2p1NXACcEfWu10vK0H8hEJtenBEDAJWU0isABsrS5QsV0j6PoWe+BLgx5seunU3TtDdQESspnAh72JJR0rqK6mXpMMk/d9st6nAv0ganl1sO5PCV/JN8ShwgKTtswuUP23dIGmkpC9ktei/USiVNLfTxh3AztnQwJ6SjgZ2A27bxJgAiIgXgE9RqLm3NQBoojDio6ekM4GBRduXATt2ZKSGpJ2B/02hzPEN4MeSSpZizFo5QXcTEXEBcCqFC38rKHwtP5HCyAYoJJH5wOPAE8DCbN2mnGsWcH3W1gI2TKoNFC6cLQFWUUiWJ7TTRiPwuWzfRgo9z89FxMpNialN2/dFRHvfDmYCd1IYevcShW8dxeWL1ptwGiUtLHeerKR0DfDziHgsIp6jMBLk6tYRMmalyBeTzczS5B60mVminKDNzBLlBG1mlignaDOzRJW6cSFXc0Ye5auX9j4Tmp7JOwRL0EuNj2/23CbrVj5fcc7pNWynmsyl4h60mVmiku1Bm5nVVEt790vlywnazAygOb3pzJ2gzcyAiJa8Q3gfJ2gzM4AWJ2gzszS5B21mlihfJDQzS5R70GZmaQqP4jAzS5QvEpqZJcolDjOzRPkioZlZotyDNjNLlC8SmpklyhcJzczSFOEatJlZmlyDNjNLlEscZmaJcg/azCxRzevyjuB9nKDNzMAlDjOzZLnEYWaWKPegzcwS5QRtZpam8EVCM7NEuQZtZpaoTixxSHoReAtoBpoiYqykIcD1wI7Ai8BXI+L1Uu00dFpEZmb1LFoqXypzYESMiYix2eczgNkRMRqYnX0uyQnazAwKPehKl01zBDAlez8FOLLcAU7QZmbQoR60pEmS5hctk9q2BtwlaUHRtpERsRQgex1RLiTXoM3MAJoqn7A/IiYDk0vssl9ELJE0Apgl6S+bEpJ70GZm0Kk16IhYkr0uB24G9gKWSRoFkL0uL9eOE7SZGXRaDVpSP0kDWt8DhwBPAtOBCdluE4BbyoXkEoeZGXTmOOiRwM2SoJBjr42IGZLmAdMkTQReBo4q15ATtJkZdNo46Ih4HtijnfWNwEEdacsJ2swMfCehmVmyOjCKo1acoM3MACLyjuB9nKDNzMDTjZqZJcsJ2swsUb5IaGaWqObmvCN4HydoMzNwicPMLFlO0GZmiXIN2swsTdHicdBmZmlyicPMLFEexWFmlij3oM3MEuUEbeU0bNGLMbecTUPvnqhHD1bc9hAvnjeNnoP6s9vkH9Jnu+G8+8oKnvr2BTStXpN3uFYj5110FuMP+RSNK1dxyP5fAmCrQQO5+PLz2Ha7D7D4lSWc8M0f8ebqt3KOtI4lOFmSH3mVmJa/reOxL53F/PGnM/+g0xkyfgwD/8dotj/pSN649wnm7nMyb9z7BNufVPaJ7daF3DB1OhO++r0N1p1wykTuv+dhxu31ee6/52FO+MHEnKLrIjrpkVedqWoJWtKukn4i6SJJv8ref6ha5+tKmte+C4B69UA9exARDDv047x2/RwAXrt+DsMO2yvHCK3W5j64gDdeX73BuoMPP5Abr5sOwI3XTeeQw8fnEVrX0RKVLzVSlQQt6SfAdYCAucC87P1USWdU45xdSkMDY2efx36LLuf1Pz/OWwv/Su/hW/He8jcAeG/5G/QaNjDnIC1vw4YPYfmylQAsX7aSYcOG5BxRnWturnypkWrVoCcCu0fEuuKVki4AFgHntneQpEnAJIBTB3yMz2+5U5XCS1xLC/MPOp2eA/uy+5Wn02/X7fKOyKzLiwQvElarxNECfKCd9aOybe2KiMkRMTYixnbb5Fyk6c21vHH/IoYcOIb3Vqym94hBAPQeMYh1K9/MOTrL28oVqxgxchgAI0YOY+XKVTlHVOe6S4kD+AEwW9KdkiZnywxgNnBKlc7ZJfQaOpCeA/sC0NCnN4MP+Chr//oqK2fOZ+ujxwGw9dHjWDljXo5RWgr+eOccvnzMFwD48jFfYNYdd+ccUZ2LlsqXGqlKiSMiZkjaGdgL2IZC/XkxMC8i0rtdJyG9Rw5i14tORD0aUINYfsuDNM5ayOr5z7L7Zaey9bHj+durK1n0rQvyDtVq6KLJP2ef/cYyeOggHnpiFr889xIu+dXlXHLFLzj6uC+y5NXX+N7xp+UdZn1LcC4ORYJj/wDmjDwqzcAsVxOansk7BEvQS42Pa3PbWHPmMRXnnH5nX7fZ56uEb1QxMwNPN2pmlqwESxxO0GZmpDnMzgnazAzcgzYzS5YTtJlZojxhv5lZmvxMQjOzVDlBm5klyqM4zMwSlWAP2k9UMTODTp/NTlIPSY9Iui37PETSLEnPZa+Dy7XhBG1mBkRzS8VLhU4Bni76fAYwOyJGU5jZs+zDS5ygzcygU3vQkrYFPgv8rmj1EcCU7P0UoOyDRZ2gzcwoDLOrdJE0SdL8omVSm+YuBH7Mhg8oGRkRSwGy1xHlYvJFQjMz6NBFwoiYDExub5ukzwHLI2KBpHGbE5ITtJkZlHgYX4ftB3xB0uFAH2CgpGuAZZJGRcRSSaOA5eUaconDzAyIppaKl5LtRPw0IraNiB2BY4A/RcTXgenAhGy3CcAt5WJyD9rMDDqzB70x5wLTJE0EXgaOKneAE7SZGdWZiyMi5gBzsveNwEEdOd4J2swMatGD7jAnaDMzPJudmVm63IM2M0tTNOUdwfs5QZuZAeEetJlZopygzczS5B60mVmi6ipBS/o1sNFxJxFxclUiMjPLQTQr7xDep1QPen7NojAzy1ld9aAjYkrxZ0n9ImJN9UMyM6u9aEmvB112NjtJ+0h6iuzRLZL2kHRJ1SMzM6uhaKl8qZVKphu9EPgM0AgQEY8BB1QzKDOzWotQxUutVDSKIyJekTYIqrk64ZiZ5aOuatBFXpG0LxCSegMns+GTas3M6l5LnY3iaPVd4FfANsCrwEzg+9UMysys1lK8SFg2QUfESuC4GsRiZpabFBN0JaM4dpJ0q6QVkpZLukXSTrUIzsysViIqX2qlklEc1wLTgFHAB4AbgKnVDMrMrNaiRRUvtVJJglZEXB0RTdlyDSVuATczq0d1NcxO0pDs7d2SzgCuo5CYjwZur0FsZmY101xnozgWUEjIrVF/p2hbAOdUKygzs1qrZc+4UqXm4vhgLQMxM8tTiqM4KrqTUNKHgd2APq3rIuKqagVlZlZrtRydUamyCVrSz4BxFBL0HcBhwH2AE7SZdRkp9qArGcXxFeAg4LWIOB7YA9iiqlGZmdVYc0tDxUutVFLieCciWiQ1SRoILAd8o4qZdSl1WeIA5ksaBFxGYWTH28DcqkZlZlZjLfU0iqNVRJyQvf2tpBnAwIh4vLphmZnVVl0Ns5P0sVLbImJhdUIyM6u9eitxnF9iWwDjOzmWDXz69Qeq2bzVqXeW3Jt3CNZF1VWJIyIOrGUgZmZ5quXojEpVdKOKmVlXl2CFo6Jx0GZmXV5LqOKlFEl9JM2V9JikRZLOytYPkTRL0nPZ6+ByMTlBm5nRqdON/g0YHxF7AGOAQyXtDZwBzI6I0cDs7HNJlTxRRZK+LunM7PP2kvYqd5yZWT1p6cBSShS8nX3slS0BHAFMydZPAY4sF1MlPehLgH2Ar2Wf3wIuruA4M7O6EajiRdIkSfOLlknFbUnqIelRCndez4qIh4GREbEUIHsdUS6mSi4SfiIiPibpkazh1yX17vBPb2aWsKYODLOLiMnA5BLbm4Ex2V3YN2czgnZYJT3odZJ6kF3klDSc8r18M7O60pEedMVtRrwBzAEOBZZJGgWQvS4vd3wlCfoi4GZghKR/pzDV6H9UHKGZWR3orBq0pOFZzxlJWwKfBv4CTAcmZLtNAG4pF1Mlc3H8QdICClOOCjgyIp4ud5yZWT3pSM+4jFHAlKzy0ABMi4jbJD0ITJM0EXgZOKpcQ5VM2L89sBa4tXhdRLy8qdGbmaWms+q22WRye7azvpFCR7dilVwkvJ2/Pzy2D/BB4Blg946cyMwsZc2d14PuNJWUOD5S/Dmb5e47G9ndzKwuJfjEq47PxRERCyV9vBrBmJnlpaUee9CSTi362AB8DFhRtYjMzHKQ4mRJlfSgBxS9b6JQk76xOuGYmeUjxZs7SibobJhI/4g4vUbxmJnlokV1VOKQ1DMimko9+srMrKtozjuAdpTqQc+lUG9+VNJ04AZgTevGiLipyrGZmdVMvY7iGAI0UngGYet46ACcoM2sy6i3URwjshEcT/L3xNwqxQueZmabLMWkVipB9wD6Q7t/VlL8WczMNlm9lTiWRsTZNYvEzCxH9TbMLsG/J2Zm1dGcYMYrlaA7NOuSmVk9q6sedESsqmUgZmZ5qqsEbWbWnXTgkYQ14wRtZoZ70GZmyaq3W73NzLqNehsHbWbWbbjEYWaWKCdoM7NEpTh/hRO0mRmuQZuZJcujOMzMEtWSYJHDCdrMDF8kNDNLVnr9ZydoMzPAPWgzs2Q1Kb0+tBO0mRkucZiZJcslDjOzRHmYnZlZotJLz07QZmZAmiWOhrwDMDNLQTNR8VKKpO0k3S3paUmLJJ2SrR8iaZak57LXweVicoI2M6PQg650KaMJOC0iPgTsDXxf0m7AGcDsiBgNzM4+l+QEbWYGRAf+K9lOxNKIWJi9fwt4GtgGOAKYku02BTiyXExO0GZmdKwHLWmSpPlFy6T22pS0I7An8DAwMiKWQiGJAyPKxeSLhInbaquBTL70F+y++y5EBN/+9mk89PCCvMOyHBzy5Qn069uXhoYGevTowbQrLuK0f/0/vPjyYgDeevttBvTvz41TLs450vrUkWF2ETEZmFxqH0n9gRuBH0TEm1LHJ5x2gk7cLy84m5kz7+boYybRq1cv+vbdMu+QLEdX/PpcBg/aav3n88/56fr35/36Mvr365tHWF1CZw6zk9SLQnL+Q0TclK1eJmlURCyVNApYXq4dlzgSNmBAfz65/ye44vdTAVi3bh2rV7+Zc1SWoohgxp/u4fCDx+UdSt1qIipeSlGhq3w58HREXFC0aTowIXs/AbilXExO0AnbaacdWLmykct/90vmzZ3Jpb89zz3obkwSk374z3z1mydxwy13bLBtwWNPMnTwYHbYbpucoqt/nXWRENgP+AYwXtKj2XI4cC5wsKTngIOzzyXVPEFLOr7EtvWF95aWNbUMK0k9e/Rgzz0/wqWXXsXH9/oMa9as5Sc/PjHvsCwnV//mfG74/X/ym/PPYepNtzH/0SfWb7tj1hwOP/hTOUZX/zprmF1E3BcRioiPRsSYbLkjIhoj4qCIGJ29rioXUx496LM2tiEiJkfE2IgY29DQr5YxJWnxq0tZvHgpc+c9AsBNN93OnmM+knNUlpcRw4cCMHTwIA46YF+eeOoZAJqamvnjnx/g0IMOyDO8uteJPehOU5WLhJIe39gmYGQ1ztkVLVu2gsWLl7Dzzv/As8/+P8aP35+nn34277AsB2vfeZdoaaFfv76sfeddHpi7kO8dfywAD81/hJ122JatRwzPOcr6luKt3tUaxTES+Azwepv1Ah6o0jm7pFN++K9cNeXX9O7dixdeeJmJ3zo175AsB42rXueUfzoHgOamZg4/ZBz77z0WgDv/+GcO+/S4HKPrGpojvemSFFUIStLlwO8j4r52tl0bEceWa6Nn723S+21Z7t5Zcm/eIViCeg3bqeODjNs4docvVpxzrn3p5s0+XyWq0oOOiIkltpVNzmZmtVbL2nKlfKOKmRndqwZtZlZX/EQVM7NEucRhZpaoFEdxOEGbmeESh5lZsnyR0MwsUa5Bm5klyiUOM7NEVeOu6s3lBG1mBjS7B21mliaXOMzMEuUSh5lZotyDNjNLlIfZmZklyrd6m5klyiUOM7NEOUGbmSXKozjMzBLlHrSZWaI8isPMLFHNkd6Eo07QZma4Bm1mlizXoM3MEuUatJlZolpc4jAzS5N70GZmifIoDjOzRLnEYWaWqBRLHA15B2BmloKWiIqXciRdIWm5pCeL1g2RNEvSc9nr4HLtOEGbmVHoQVf6XwWuBA5ts+4MYHZEjAZmZ59LcoI2MwOao7nipZyIuAdY1Wb1EcCU7P0U4Mhy7ThBm5lRuNW70kXSJEnzi5ZJFZxiZEQszc61FBhR7gBfJDQzo2O3ekfEZGBy9aIpcII2M6MmkyUtkzQqIpZKGgUsL3eASxxmZnTuKI6NmA5MyN5PAG4pd4B70GZmdO44aElTgXHAMEmLgZ8B5wLTJE0EXgaOKteOE7SZGZ17q3dEfG0jmw7qSDtO0GZmeMJ+M7NkeS4OM7NEuQdtZpYoP/LKzCxR7kGbmSXKE/abmSXKFwnNzBLlEoeZWaJSfKKKE7SZGe5Bm5klK8UatFL8q2EbkjQpm3/WbD3/u+j6PN1ofajkaQ3W/fjfRRfnBG1mlignaDOzRDlB1wfXGa09/nfRxfkioZlZotyDNjNLlBO0mVminKATJ+lQSc9I+qukM/KOx/In6QpJyyU9mXcsVl1O0AmT1AO4GDgM2A34mqTd8o3KEnAlcGjeQVj1OUGnbS/grxHxfES8B1wHHJFzTJaziLgHWJV3HFZ9TtBp2wZ4pejz4mydmXUDTtBpUzvrPC7SrJtwgk7bYmC7os/bAktyisXMaswJOm3zgNGSPiipN3AMMD3nmMysRpygExYRTcCJwEzgaWBaRCzKNyrLm6SpwIPALpIWS5qYd0xWHb7V28wsUe5Bm5klygnazCxRTtBmZolygjYzS5QTtJlZopygrSRJzZIelfSkpBsk9d2Mtq6U9JXs/e9KTfwkaZykfTfhHC9KGlbp+jb7vN3Bc/2bpB91NEazSjlBWznvRMSYiPgw8B7w3eKN2Yx7HRYR34qIp0rsMg7ocII260qcoK0j7gX+Mevd3i3pWuAJST0knSdpnqTHJX0HQAX/KekpSbcDI1obkjRH0tjs/aGSFkp6TNJsSTtS+EPww6z3/klJwyXdmJ1jnqT9smOHSrpL0iOSLqX9+Us2IOm/JS2QtEjSpDbbzs9imS1peLbuHyTNyI65V9KunfHLNCunZ94BWH2Q1JPCvNQzslV7AR+OiBeyJLc6Ij4uaQvgfkl3AXsCuwAfAUYCTwFXtGl3OHAZcEDW1pCIWCXpt8DbEfGLbL9rgV9GxH2Stqdwd+WHgJ8B90XE2ZI+C2yQcDfim9k5tgTmSboxIhqBfsDCiDhN0plZ2ydSeDjrdyPiOUmfAC4Bxm/Cr9GsQ5ygrZwtJT2avb8XuJxC6WFuRLyQrT8E+GhrfRnYChgNHABMjYhmYImkP7XT/t7APa1tRcTG5jn+NLCbtL6DPFDSgOwcX8qOvV3S6xX8TCdL+mL2frss1kagBbg+W38NcJOk/tnPe0PRubeo4Bxmm80J2sp5JyLGFK/IEtWa4lXASRExs81+h1N+elRVsA8UynH7RMQ77cRS8XwFksZRSPb7RMRaSXOAPhvZPbLzvtH2d2BWC65BW2eYCXxPUi8ASTtL6gfcAxyT1ahHAQe2c+yDwKckfTA7dki2/i1gQNF+d1EoN5Dt15ow7wGOy9YdBgwuE+tWwOtZct6VQg++VQPQ+i3gWAqlkzeBFyQdlZ1DkvYocw6zTuEEbZ3hdxTqywuzB5leSuHb2c3Ac8ATwG+AP7c9MCJWUKgb3yTpMf5eYrgV+GLrRULgZGBsdhHyKf4+muQs4ABJCymUWl4uE+sMoKekx4FzgIeKtq0Bdpe0gEKN+exs/XHAxCy+RfixY1Yjns3OzCxR7kGbmSXKCdrMLFFO0GZmiXKCNjNLlBO0mVminKDNzBLlBG1mlqj/D4MNcAGhiliEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "logistic_cm = confusion_matrix(y_test, model1.predict(x_test))\n",
    "sns.heatmap(logistic_cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b80b7",
   "metadata": {},
   "source": [
    "### Train the model with embeddings(questions + answers) as input and grades as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c9b37c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_answers = []\n",
    "grades = []\n",
    "for (q,i) in zip(list(QA.keys()), QA.values()):\n",
    "    temp1 = list(i.keys())\n",
    "    temp2 = list(i.values())\n",
    "    for (j,k) in zip(temp1,temp2):\n",
    "        questions_answers.append(remove_stop_words(str(q)) + \" \" + remove_stop_words(str(j)))\n",
    "        grades.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2a1be651",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer()\n",
    "answer_vectors2 = vectorizer2.fit_transform(questions_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "06761626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1280x1627 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 25610 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_vectors2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cc9db013",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MLPClassifier(hidden_layer_sizes = (64,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "673d1b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(64, 32))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(answer_vectors2, grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0b7dbb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model2.predict(answer_vectors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684bd2a",
   "metadata": {},
   "source": [
    "### Train the model with concat(embeddings(questions), embeddings(answers)) as input and grades as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52f4231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d5b48cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sentence):\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    processed_sentence = ' '.join(words)\n",
    "    return processed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "db5df658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vec(sentence, embedding_model, num_features):\n",
    "    words = sentence.split()\n",
    "    sentence_embedding = np.zeros((num_features,), dtype=\"float32\")\n",
    "    num_words = 0\n",
    "    for word in words:\n",
    "#       if word in embedding_model.key_to_index_dict:\n",
    "        sentence_embedding = np.add(sentence_embedding, embedding_model[word])\n",
    "        sentence_embedding\n",
    "        num_words += 1\n",
    "    if num_words > 0:\n",
    "        sentence_embedding = np.divide(sentence_embedding, num_words)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "34fe4c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "# # Convert the sentence to a vector using the pre-trained GloVe model\n",
    "# vector = sentence_to_vec(remove_stop_words(sentence), model, 100)\n",
    "# vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1e891e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e23605ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import nlp\n",
    "\n",
    "# # Load the pre-trained GloVe model\n",
    "# model = nlp.load('glove-100d')\n",
    "\n",
    "# # Define a function to convert a sentence to a vector using the pre-trained GloVe model\n",
    "# def sentence_to_vec(sentence, model, embedding_size):\n",
    "#     words = sentence.lower().split()\n",
    "#     # Initialize a vector of zeros\n",
    "#     sentence_vec = np.zeros((embedding_size,))\n",
    "#     # Loop over the words in the sentence and add their vectors to the sentence vector\n",
    "#     for word in words:\n",
    "#         if word in model.vocab:\n",
    "#             sentence_vec += model[word]\n",
    "#     return sentence_vec\n",
    "\n",
    "# # Example usage\n",
    "# sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "# vector = sentence_to_vec(sentence, model, 100)\n",
    "# print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f38840b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc044bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-098a3d906b0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Print information about the GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m     \"\"\"\n\u001b[1;32m--> 341\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \"\"\"\n\u001b[1;32m--> 371\u001b[1;33m     \u001b[0m_lazy_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# will define _get_device_properties\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check CUDA version\n",
    "print(torch.version.cuda)\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ab662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
